"""
ç®€å•çš„æƒé‡å­¦ä¹ ç¯å¢ƒ
ä½¿ç”¨Stable-Baselines3è®­ç»ƒAPFæƒé‡ç³»æ•°
"""
import numpy as np
import gym
from gym import spaces
import os
from dqn_reward_config_data import DQNRewardConfig


class SimpleWeightEnv(gym.Env):
    """
    ç®€å•çš„APFæƒé‡å­¦ä¹ ç¯å¢ƒ
    
    ç›®æ ‡: å­¦ä¹ 5ä¸ªæƒé‡ç³»æ•° (Î±1, Î±2, Î±3, Î±4, Î±5)
    """
    
    def __init__(self, server=None, drone_name="UAV1", reward_config_path=None, reset_unity=True, step_duration=5.0):
        super(SimpleWeightEnv, self).__init__()
        
        self.server = server
        self.drone_name = drone_name
        self.reset_unity = reset_unity  # æ˜¯å¦æ¯æ¬¡episodeé‡ç½®Unityç¯å¢ƒ
        self.step_duration = step_duration  # æ¯æ­¥é£è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
        
        # åŠ è½½å¥–åŠ±é…ç½®
        if reward_config_path is None:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„
            current_dir = os.path.dirname(os.path.abspath(__file__))
            reward_config_path = os.path.join(current_dir, "dqn_reward_config.json")
        
        self.reward_config = DQNRewardConfig(reward_config_path)
        print(f"[OK] DQNç¯å¢ƒå·²åŠ è½½å¥–åŠ±é…ç½®")
        
        # çŠ¶æ€ç©ºé—´: 18ç»´
        # [ä½ç½®(3) + é€Ÿåº¦(3) + æ–¹å‘(3) + ç†µå€¼(3) + Leader(3) + æ‰«æ(3)]
        self.observation_space = spaces.Box(
            low=-100.0,
            high=100.0,
            shape=(18,),
            dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´: 5ç»´è¿ç»­ï¼ˆ5ä¸ªæƒé‡ç³»æ•°ï¼‰
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„èŒƒå›´
        self.action_space = spaces.Box(
            low=self.reward_config.weight_min,
            high=self.reward_config.weight_max,
            shape=(5,),
            dtype=np.float32
        )
        
        # è®°å½•ä¸Šä¸€æ­¥çš„çŠ¶æ€
        self.prev_scanned_cells = 0
        self.step_count = 0
        self.episode_count = 0  # è®°å½•Episodeç¼–å·
        self.last_action = np.zeros(5)  # è®°å½•ä¸Šä¸€æ­¥çš„åŠ¨ä½œï¼Œç”¨äºç”µé‡æ¶ˆè€—è®¡ç®—
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        import time
        import sys
        
        # Episodeè®¡æ•°
        self.episode_count += 1
        
        print(f"\n{'='*60}")
        print(f"ğŸ”„ é‡ç½®ç¯å¢ƒ - Episode #{self.episode_count}")
        print(f"{'='*60}")
        
        # å¦‚æœæœ‰server
        if self.server:
            # é‡ç½®ç”µé‡æ•°æ®
            if self.reset_unity:
                print(f"ğŸ”‹ é‡ç½®ç”µé‡æ•°æ®...")
                self.server.reset_battery_voltage(self.drone_name)
                print(f"  âœ… ç”µé‡å·²é‡ç½®ä¸º4.2V")
            
            # æ¨¡å¼Aï¼šæ ‡å‡†episodeè®­ç»ƒï¼ˆé‡ç½®Unityç¯å¢ƒï¼‰
            if self.reset_unity:
                print(f"ğŸ® æ­£åœ¨é‡ç½®Unityç¯å¢ƒ...")
                self.server.reset_environment()
                
                # ç­‰å¾…é‡ç½®å®Œæˆ
                for i in range(3):
                    sys.stdout.write(f"\r  â³ ç­‰å¾…é‡ç½®... {'.' * (i+1)}   ")
                    sys.stdout.flush()
                    time.sleep(1)
                print(f"\r  âœ… Unityé‡ç½®å®Œæˆ!     ")
            
            # ç­‰å¾…æ•°æ®å°±ç»ª
            print(f"\nğŸ“¡ ç­‰å¾…æ•°æ®åŒæ­¥...")
            max_wait = 10
            wait_time = 0
            while wait_time < max_wait:
                has_grid = bool(self.server.grid_data.cells)
                has_runtime = bool(self.server.unity_runtime_data.get(self.drone_name))
                
                if has_grid and has_runtime:
                    grid_count = len(self.server.grid_data.cells)
                    print(f"âœ… æ•°æ®å°±ç»ªï¼")
                    print(f"  ğŸ—ºï¸  ç½‘æ ¼å•å…ƒ: {grid_count} ä¸ª")
                    print(f"  ğŸš æ— äººæœº: {self.drone_name}")
                    break
                
                dots = '.' * (int(wait_time * 2) % 4)
                sys.stdout.write(f"\r  ç­‰å¾…æ•°æ®{dots}    ")
                sys.stdout.flush()
                time.sleep(0.5)
                wait_time += 0.5
            
            if wait_time >= max_wait:
                print(f"\r  âš ï¸  ç­‰å¾…æ•°æ®è¶…æ—¶     ")
        
        # é‡ç½®å†…éƒ¨çŠ¶æ€
        if self.reset_unity:
            self.prev_scanned_cells = 0
        else:
            if self.server:
                with self.server.data_lock:
                    self.prev_scanned_cells = self._count_scanned_cells()
            else:
                self.prev_scanned_cells = 0
        
        self.step_count = 0
        self.last_action = np.zeros(5)
        
        state = self._get_state()
        
        # æ˜¾ç¤ºç”µé‡ä¿¡æ¯
        if self.server:
            current_voltage = self.server.get_battery_voltage(self.drone_name)
            print(f"ğŸ”‹ å½“å‰ç”µé‡: {current_voltage:.2f}V")
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ å¼€å§‹ Episode #{self.episode_count}")
        print(f"{'='*60}")
        print(f"ğŸ“Š é…ç½®:")
        print(f"  â€¢ Episodeç¼–å·: #{self.episode_count}")
        print(f"  â€¢ æœ€å¤§æ­¥æ•°: {self.reward_config.max_steps}")
        print(f"  â€¢ æ¯æ­¥æ—¶é•¿: {self.step_duration}ç§’")
        print(f"  â€¢ é¢„è®¡æ—¶é•¿: {self.reward_config.max_steps * self.step_duration / 60:.1f}åˆ†é’Ÿ")
        print(f"{'='*60}\n")
        
        return state
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥
        
        :param action: [Î±1, Î±2, Î±3, Î±4, Î±5] - 5ä¸ªæƒé‡ç³»æ•°
        :return: observation, reward, done, info
        """
        import time
        import sys
        
        # ç¡®ä¿actionåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, self.reward_config.weight_min, self.reward_config.weight_max)
        
        # å°†æƒé‡è®¾ç½®åˆ°APFç®—æ³•
        weights = {
            'repulsionCoefficient': float(action[0]),
            'entropyCoefficient': float(action[1]),
            'distanceCoefficient': float(action[2]),
            'leaderRangeCoefficient': float(action[3]),
            'directionRetentionCoefficient': float(action[4])
        }
        
        # æ‰“å°å½“å‰æ­¥éª¤ä¿¡æ¯
        self.step_count += 1
        progress_percent = (self.step_count / self.reward_config.max_steps) * 100
        
        print(f"\n{'â”€'*60}")
        print(f"ğŸ”„ æ­¥éª¤ {self.step_count}/{self.reward_config.max_steps} ({progress_percent:.1f}%)")
        print(f"{'â”€'*60}")
        print(f"ğŸ“Š è®¾ç½®æƒé‡:")
        print(f"  â€¢ æ–¥åŠ›ç³»æ•°: {weights['repulsionCoefficient']:.3f}")
        print(f"  â€¢ ç†µç³»æ•°:   {weights['entropyCoefficient']:.3f}")
        print(f"  â€¢ è·ç¦»ç³»æ•°: {weights['distanceCoefficient']:.3f}")
        print(f"  â€¢ Leader:   {weights['leaderRangeCoefficient']:.3f}")
        print(f"  â€¢ æ–¹å‘ä¿æŒ: {weights['directionRetentionCoefficient']:.3f}")
        
        # åœ¨ step() æ–¹æ³•ä¸­
        if self.server:
            # æ›´æ–°ç”µé‡æ¶ˆè€—ï¼ˆä½¿ç”¨æ–°çš„ç”µé‡æ¨¡å—ï¼‰
            if self.step_count > 1:
                action_intensity = np.linalg.norm(action - self.last_action)
                self.server.battery_manager.update_voltage(self.drone_name, action_intensity)
            
            # æ˜¾ç¤ºå½“å‰ç”µé‡
            battery_info = self.server.battery_manager.get_battery_info(self.drone_name)
            current_voltage = battery_info.voltage
            print(f"ğŸ”‹ å½“å‰ç”µé‡: {current_voltage:.2f}V ({battery_info.get_remaining_percentage():.1f}%)")
            
            # è®¾ç½®æƒé‡ï¼ˆç®—æ³•çº¿ç¨‹ä¼šä½¿ç”¨æ–°æƒé‡é£è¡Œï¼‰
            self.server.algorithms[self.drone_name].set_coefficients(weights)
            
            # å€’è®¡æ—¶ç­‰å¾…æ— äººæœºé£è¡Œ
            print(f"\nâ±ï¸  ç­‰å¾…æ— äººæœºé£è¡Œ {self.step_duration:.0f} ç§’...")
            
            # ä½¿ç”¨å€’è®¡æ—¶æ˜¾ç¤º
            for remaining in range(int(self.step_duration), 0, -1):
                elapsed = self.step_duration - remaining
                bar_length = 40
                filled = int((elapsed / self.step_duration) * bar_length)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                
                sys.stdout.write(f"\r  [{bar}] {remaining:2d}ç§’å‰©ä½™  ")
                sys.stdout.flush()
                time.sleep(1)
            
            print(f"\r  [{'â–ˆ'*40}] âœ… å®Œæˆ!     ")
        else:
            time.sleep(0.1)  # æµ‹è¯•æ¨¡å¼å¿«é€Ÿè·³è¿‡
        
        # è®°å½•å½“å‰åŠ¨ä½œ
        self.last_action = action.copy()
        
        # è·å–æ–°çŠ¶æ€
        next_state = self._get_state()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward()
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.step_count >= self.reward_config.max_steps
        
        # æ˜¾ç¤ºå¥–åŠ±ä¿¡æ¯
        print(f"\nğŸ“ˆ æœ¬æ­¥å¥–åŠ±: {reward:+.2f}")
        
        if self.server:
            with self.server.data_lock:
                grid_data = self.server.grid_data
                if grid_data and grid_data.cells:
                    total_cells = len(grid_data.cells)
                    scanned_cells = sum(1 for cell in grid_data.cells if cell.entropy < 30)
                    scan_progress = (scanned_cells / total_cells) * 100
                    print(f"ğŸ—ºï¸  æ‰«æè¿›åº¦: {scanned_cells}/{total_cells} ({scan_progress:.1f}%)")
        
        if done:
            print(f"\n{'='*60}")
            print(f"âœ… Episode #{self.episode_count} å®Œæˆï¼å…± {self.step_count} æ­¥")
            print(f"{'='*60}")
            print(f"ğŸ”„ å³å°†è‡ªåŠ¨é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªEpisode...")
            print(f"{'='*60}\n")
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'weights': weights,
            'scanned_cells': self.prev_scanned_cells
        }
        
        return next_state, reward, done, info
    
    def _get_state(self):
        """è·å–å½“å‰çŠ¶æ€ï¼ˆ18ç»´ï¼‰"""
        if not self.server:
            # å¦‚æœæ²¡æœ‰serverï¼Œè¿”å›éšæœºçŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            return np.random.randn(18).astype(np.float32)
        
        try:
            with self.server.data_lock:
                runtime_data = self.server.unity_runtime_data[self.drone_name]
                grid_data = self.server.grid_data
                
                # 1. ä½ç½® (3)
                pos = runtime_data.position
                position = [pos.x, pos.y, pos.z]
                
                # 2. é€Ÿåº¦ (3)
                vel = runtime_data.finalMoveDir
                velocity = [
                    vel.x * self.server.config_data.moveSpeed,
                    vel.y * self.server.config_data.moveSpeed,
                    vel.z * self.server.config_data.moveSpeed
                ]
                
                # 3. æ–¹å‘ (3)
                fwd = runtime_data.forward
                direction = [fwd.x, fwd.y, fwd.z]
                
                # 4. é™„è¿‘ç†µå€¼ (3)
                entropy_info = self._get_entropy_info(grid_data, pos)
                
                # 5. Leaderç›¸å¯¹ä½ç½® (3)
                if runtime_data.leader_position:
                    leader_rel = [
                        runtime_data.leader_position.x - pos.x,
                        runtime_data.leader_position.y - pos.y,
                        runtime_data.leader_position.z - pos.z
                    ]
                else:
                    leader_rel = [0.0, 0.0, 0.0]
                
                # 6. æ‰«æè¿›åº¦ (3)
                scan_info = self._get_scan_info(grid_data)
                
                # ç»„åˆçŠ¶æ€
                state = position + velocity + direction + entropy_info + leader_rel + scan_info
                
                return np.array(state, dtype=np.float32)
                
        except Exception as e:
            print(f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")
            return np.zeros(18, dtype=np.float32)
    
    def _calculate_reward(self):
        """è®¡ç®—å¥–åŠ±ï¼ˆåŒ…å«ç”µé‡å¥–åŠ±æœºåˆ¶ï¼‰"""
        if not self.server:
            return 0.0
        
        reward = 0.0
        
        try:
            with self.server.data_lock:
                runtime_data = self.server.unity_runtime_data[self.drone_name]
                grid_data = self.server.grid_data
                
                # 1. æ¢ç´¢å¥–åŠ±ï¼šæ–°æ‰«æçš„å•å…ƒæ ¼
                current_scanned = sum(1 for cell in grid_data.cells if cell.entropy < 30) if grid_data.cells else 0
                new_scanned = current_scanned - self.prev_scanned_cells
                reward += new_scanned * self.reward_config.exploration_reward
                self.prev_scanned_cells = current_scanned
                
                # 2. è¶Šç•Œæƒ©ç½š
                if runtime_data.leader_position:
                    dist_to_leader = (runtime_data.position - runtime_data.leader_position).magnitude()
                    if runtime_data.leader_scan_radius > 0 and dist_to_leader > runtime_data.leader_scan_radius:
                        reward -= self.reward_config.out_of_range_penalty
                
                # 3. ç”µé‡å¥–åŠ±æœºåˆ¶
                current_voltage = self.server.get_battery_voltage(self.drone_name)
                
                # æœ€ä¼˜ç”µé‡èŒƒå›´å¥–åŠ± (3.7V - 4.0V)
                if 3.7 <= current_voltage <= 4.0:
                    reward += self.reward_config.battery_optimal_reward
                    print(f"ğŸ”‹ ç”µé‡å¥–åŠ±: +{self.reward_config.battery_optimal_reward:.2f} (ç”µé‡{current_voltage:.2f}Våœ¨æœ€ä¼˜èŒƒå›´)")
                
                # ä½ç”µé‡æƒ©ç½š (ä½äº3.5V)
                elif current_voltage < 3.5:
                    reward -= self.reward_config.battery_low_penalty
                    print(f"ğŸ”‹ ç”µé‡æƒ©ç½š: -{self.reward_config.battery_low_penalty:.2f} (ç”µé‡{current_voltage:.2f}Vè¿‡ä½)")
                
                # 4. åŠ¨ä½œå¹³ç¨³å¥–åŠ±ï¼ˆå‡å°‘å‰§çƒˆåŠ¨ä½œï¼‰
                if self.step_count > 1:
                    action_intensity = np.linalg.norm(self.last_action)
                    if action_intensity < 0.5:  # åŠ¨ä½œå¼ºåº¦å°äº0.5æ—¶ç»™äºˆå¥–åŠ±
                        reward += self.reward_config.action_smooth_reward
                        print(f"ğŸ”„ å¹³ç¨³åŠ¨ä½œå¥–åŠ±: +{self.reward_config.action_smooth_reward:.2f}")
                
        except Exception as e:
            print(f"[é”™è¯¯] è®¡ç®—å¥–åŠ±å¤±è´¥: {str(e)}")
        
        return reward
    
    def _get_entropy_info(self, grid_data, position):
        """è·å–é™„è¿‘ç†µå€¼ä¿¡æ¯"""
        if not grid_data or not grid_data.cells:
            return [50.0, 50.0, 0.0]
        
        # æ‰¾é™„è¿‘10ç±³å†…çš„å•å…ƒæ ¼
        nearby_cells = [
            cell for cell in grid_data.cells[:100]
            if (cell.center - position).magnitude() < 10.0
        ]
        
        if not nearby_cells:
            return [50.0, 50.0, 0.0]
        
        entropies = [cell.entropy for cell in nearby_cells]
        return [
            float(np.mean(entropies)),
            float(np.max(entropies)),
            float(np.std(entropies))
        ]
    
    def _get_scan_info(self, grid_data):
        """è·å–æ‰«æè¿›åº¦"""
        if not grid_data or not grid_data.cells:
            return [0.0, 0.0, 0.0]
        
        total = len(grid_data.cells)
        scanned = sum(1 for cell in grid_data.cells if cell.entropy < 30)
        
        return [
            scanned / max(total, 1),
            float(scanned),
            float(total - scanned)
        ]
    
    def _count_scanned_cells(self):
        """ç»Ÿè®¡å·²æ‰«æå•å…ƒæ ¼ï¼ˆä¸åŠ é”ç‰ˆæœ¬ï¼Œç”±è°ƒç”¨è€…åŠ é”ï¼‰"""
        if not self.server or not self.server.grid_data:
            return 0
        
        try:
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡ŒåŠ é”ï¼Œé¿å…åµŒå¥—é”
            # è°ƒç”¨è€…åº”è¯¥å·²ç»æŒæœ‰data_lock
            return sum(1 for cell in self.server.grid_data.cells if cell.entropy < 30)
        except:
            return 0


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•SimpleWeightEnv...")
    
    # æµ‹è¯•ä¸¤ç§æ¨¡å¼
    print("\n[æ¨¡å¼A] æ ‡å‡†episodeè®­ç»ƒ:")
    env_a = SimpleWeightEnv(server=None, drone_name="UAV1", reset_unity=True)
    print(f"  è§‚å¯Ÿç©ºé—´: {env_a.observation_space.shape}")
    print(f"  åŠ¨ä½œç©ºé—´: {env_a.action_space.shape}")
    
    print("\n[æ¨¡å¼B] è¿ç»­å­¦ä¹ :")
    env_b = SimpleWeightEnv(server=None, drone_name="UAV1", reset_unity=False)
    print(f"  è§‚å¯Ÿç©ºé—´: {env_b.observation_space.shape}")
    print(f"  åŠ¨ä½œç©ºé—´: {env_b.action_space.shape}")
    
    print("\n[OK] ä¸¤ç§æ¨¡å¼éƒ½å¯ç”¨ï¼")