"""
ç®€å•çš„æƒé‡å­¦ä¹ ç¯å¢ƒ
ä½¿ç”¨Stable-Baselines3è®­ç»ƒAPFæƒé‡ç³»æ•°
"""
import numpy as np
import gym
from gym import spaces
import os

try:
    from .crazyflie_reward_config import CrazyflieRewardConfig
except ImportError:
    from crazyflie_reward_config import CrazyflieRewardConfig


class SimpleWeightEnv(gym.Env):
    """
    ç®€å•çš„APFæƒé‡å­¦ä¹ ç¯å¢ƒ
    
    ç›®æ ‡: å­¦ä¹ 5ä¸ªæƒé‡ç³»æ•° (Î±1, Î±2, Î±3, Î±4, Î±5)
    """
    
    def __init__(
        self,
        server=None,
        drone_name="UAV1",
        reward_config_path=None,
        reset_unity=True,
        step_duration=5.0,
        safety_limit=True,
        max_weight_delta=0.5
    ):
        super(SimpleWeightEnv, self).__init__()
        
        self.server = server
        self.drone_name = drone_name
        self.reset_unity = reset_unity  # æ˜¯å¦æ¯æ¬¡episodeé‡ç½®Unityç¯å¢ƒ
        self.step_duration = step_duration  # æ¯æ­¥é£è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
        
        # åŠ è½½å¥–åŠ±é…ç½®ï¼ˆä¸å®ä½“è®­ç»ƒä¸€è‡´ï¼‰
        if reward_config_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            reward_config_path = os.path.join(current_dir, "crazyflie_reward_config.json")

        self.reward_config = CrazyflieRewardConfig(reward_config_path)
        print("[OK] è®­ç»ƒç¯å¢ƒå·²åŠ è½½å¥–åŠ±é…ç½®ï¼ˆä¸å®ä½“ä¸€è‡´ï¼‰")
        
        # çŠ¶æ€ç©ºé—´: 18ç»´
        # [ä½ç½®(3) + é€Ÿåº¦(3) + æ–¹å‘(3) + ç†µå€¼(3) + Leader(3) + æ‰«æ(3)]
        self.observation_space = spaces.Box(
            low=-100.0,
            high=100.0,
            shape=(18,),
            dtype=np.float32
        )
        
        # åŠ¨ä½œç©ºé—´: 5ç»´è¿ç»­ï¼ˆ5ä¸ªæƒé‡ç³»æ•°ï¼‰
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„èŒƒå›´
        self.action_space = spaces.Box(
            low=self.reward_config.weight_min,
            high=self.reward_config.weight_max,
            shape=(5,),
            dtype=np.float32
        )
        
        # è®°å½•ä¸Šä¸€æ­¥çš„çŠ¶æ€
        self.prev_scanned_cells = 0
        self.step_count = 0
        self.episode_count = 0  # è®°å½•Episodeç¼–å·
        self.last_action = np.zeros(5)  # è®°å½•ä¸Šä¸€æ­¥çš„åŠ¨ä½œï¼Œç”¨äºç”µé‡æ¶ˆè€—è®¡ç®—
        self.prev_velocity = np.zeros(3, dtype=np.float32)
        self.prev_direction = np.array([1.0, 0.0, 0.0], dtype=np.float32)
        self.safety_limit = safety_limit
        self.max_weight_delta = max_weight_delta
        self._has_initial_action = False
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        import time
        import sys
        
        # Episodeè®¡æ•°
        self.episode_count += 1
        
        print(f"\n{'='*60}")
        print(f"ğŸ”„ é‡ç½®ç¯å¢ƒ - Episode #{self.episode_count}")
        print(f"{'='*60}")
        
        # å¦‚æœæœ‰server
        if self.server:
            # é‡ç½®ç”µé‡æ•°æ®
            if self.reset_unity:
                print(f"ğŸ”‹ é‡ç½®ç”µé‡æ•°æ®...")
                self.server.reset_battery_voltage(self.drone_name)
                print(f"  âœ… ç”µé‡å·²é‡ç½®ä¸º4.2V")
            
            # æ¨¡å¼Aï¼šæ ‡å‡†episodeè®­ç»ƒï¼ˆé‡ç½®Unityç¯å¢ƒï¼‰
            if self.reset_unity:
                print(f"ğŸ® æ­£åœ¨é‡ç½®Unityç¯å¢ƒ...")
                self.server.reset_environment()
                
                # ç­‰å¾…é‡ç½®å®Œæˆ
                for i in range(3):
                    sys.stdout.write(f"\r  â³ ç­‰å¾…é‡ç½®... {'.' * (i+1)}   ")
                    sys.stdout.flush()
                    time.sleep(1)
                print(f"\r  âœ… Unityé‡ç½®å®Œæˆ!     ")
            
            # ç­‰å¾…æ•°æ®å°±ç»ª
            print(f"\nğŸ“¡ ç­‰å¾…æ•°æ®åŒæ­¥...")
            max_wait = 10
            wait_time = 0
            while wait_time < max_wait:
                has_grid = bool(self.server.grid_data.cells)
                has_runtime = bool(self.server.unity_runtime_data.get(self.drone_name))
                
                if has_grid and has_runtime:
                    grid_count = len(self.server.grid_data.cells)
                    print(f"âœ… æ•°æ®å°±ç»ªï¼")
                    print(f"  ğŸ—ºï¸  ç½‘æ ¼å•å…ƒ: {grid_count} ä¸ª")
                    print(f"  ğŸš æ— äººæœº: {self.drone_name}")
                    break
                
                dots = '.' * (int(wait_time * 2) % 4)
                sys.stdout.write(f"\r  ç­‰å¾…æ•°æ®{dots}    ")
                sys.stdout.flush()
                time.sleep(0.5)
                wait_time += 0.5
            
            if wait_time >= max_wait:
                print(f"\r  âš ï¸  ç­‰å¾…æ•°æ®è¶…æ—¶     ")
        
        # é‡ç½®å†…éƒ¨çŠ¶æ€
        if self.reset_unity:
            self.prev_scanned_cells = 0
        else:
            if self.server:
                with self.server.data_lock:
                    self.prev_scanned_cells = self._count_scanned_cells()
            else:
                self.prev_scanned_cells = 0
        
        self.step_count = 0
        self.last_action = np.zeros(5)
        self.prev_velocity = np.zeros(3, dtype=np.float32)
        self.prev_direction = np.array([1.0, 0.0, 0.0], dtype=np.float32)
        self._has_initial_action = False
        
        state = self._get_state()
        
        # æ˜¾ç¤ºç”µé‡ä¿¡æ¯
        if self.server:
            current_voltage = self.server.get_battery_voltage(self.drone_name)
            print(f"ğŸ”‹ å½“å‰ç”µé‡: {current_voltage:.2f}V")
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ å¼€å§‹ Episode #{self.episode_count}")
        print(f"{'='*60}")
        print(f"ğŸ“Š é…ç½®:")
        print(f"  â€¢ Episodeç¼–å·: #{self.episode_count}")
        print(f"  â€¢ æœ€å¤§æ­¥æ•°: {self.reward_config.max_steps}")
        print(f"  â€¢ æ¯æ­¥æ—¶é•¿: {self.step_duration}ç§’")
        print(f"  â€¢ é¢„è®¡æ—¶é•¿: {self.reward_config.max_steps * self.step_duration / 60:.1f}åˆ†é’Ÿ")
        print(f"{'='*60}\n")
        
        return state
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥
        
        :param action: [Î±1, Î±2, Î±3, Î±4, Î±5] - 5ä¸ªæƒé‡ç³»æ•°
        :return: observation, reward, done, info
        """
        import time
        import sys
        
        # ç¡®ä¿actionåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, self.reward_config.weight_min, self.reward_config.weight_max)
        if self.safety_limit and (self.step_count > 0 or self._has_initial_action):
            action = np.clip(
                action,
                self.last_action - self.max_weight_delta,
                self.last_action + self.max_weight_delta
            )
            action = np.clip(action, self.reward_config.weight_min, self.reward_config.weight_max)
        self._has_initial_action = False
        
        # å°†æƒé‡è®¾ç½®åˆ°APFç®—æ³•
        weights = {
            'repulsionCoefficient': float(action[0]),
            'entropyCoefficient': float(action[1]),
            'distanceCoefficient': float(action[2]),
            'leaderRangeCoefficient': float(action[3]),
            'directionRetentionCoefficient': float(action[4])
        }
        
        # æ‰“å°å½“å‰æ­¥éª¤ä¿¡æ¯
        self.step_count += 1
        progress_percent = (self.step_count / self.reward_config.max_steps) * 100
        
        print(f"\n{'â”€'*60}")
        print(f"ğŸ”„ æ­¥éª¤ {self.step_count}/{self.reward_config.max_steps} ({progress_percent:.1f}%)")
        print(f"{'â”€'*60}")
        print(f"ğŸ“Š è®¾ç½®æƒé‡:")
        print(f"  â€¢ æ–¥åŠ›ç³»æ•°: {weights['repulsionCoefficient']:.3f}")
        print(f"  â€¢ ç†µç³»æ•°:   {weights['entropyCoefficient']:.3f}")
        print(f"  â€¢ è·ç¦»ç³»æ•°: {weights['distanceCoefficient']:.3f}")
        print(f"  â€¢ Leader:   {weights['leaderRangeCoefficient']:.3f}")
        print(f"  â€¢ æ–¹å‘ä¿æŒ: {weights['directionRetentionCoefficient']:.3f}")
        
        # åœ¨ step() æ–¹æ³•ä¸­
        if self.server:
            # æ›´æ–°ç”µé‡æ¶ˆè€—ï¼ˆä½¿ç”¨æ–°çš„ç”µé‡æ¨¡å—ï¼‰
            if self.step_count > 1:
                action_intensity = np.linalg.norm(action - self.last_action)
                self.server.battery_manager.update_voltage(self.drone_name, action_intensity)
            
            # æ˜¾ç¤ºå½“å‰ç”µé‡
            battery_info = self.server.battery_manager.get_battery_info(self.drone_name)
            current_voltage = battery_info.voltage
            print(f"ğŸ”‹ å½“å‰ç”µé‡: {current_voltage:.2f}V ({battery_info.get_remaining_percentage():.1f}%)")
            
            # è®¾ç½®æƒé‡ï¼ˆç®—æ³•çº¿ç¨‹ä¼šä½¿ç”¨æ–°æƒé‡é£è¡Œï¼‰
            self.server.algorithms[self.drone_name].set_coefficients(weights)
            
            # å€’è®¡æ—¶ç­‰å¾…æ— äººæœºé£è¡Œ
            print(f"\nâ±ï¸  ç­‰å¾…æ— äººæœºé£è¡Œ {self.step_duration:.0f} ç§’...")
            
            # ä½¿ç”¨å€’è®¡æ—¶æ˜¾ç¤º
            for remaining in range(int(self.step_duration), 0, -1):
                elapsed = self.step_duration - remaining
                bar_length = 40
                filled = int((elapsed / self.step_duration) * bar_length)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                
                sys.stdout.write(f"\r  [{bar}] {remaining:2d}ç§’å‰©ä½™  ")
                sys.stdout.flush()
                time.sleep(1)
            
            print(f"\r  [{'â–ˆ'*40}] âœ… å®Œæˆ!     ")
        else:
            time.sleep(0.1)  # æµ‹è¯•æ¨¡å¼å¿«é€Ÿè·³è¿‡
        
        # è·å–æ–°çŠ¶æ€
        next_state = self._get_state()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action)
        
        # è®°å½•å½“å‰åŠ¨ä½œ
        self.last_action = action.copy()
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸ
        done = self.step_count >= self.reward_config.max_steps
        
        # æ˜¾ç¤ºå¥–åŠ±ä¿¡æ¯
        print(f"\nğŸ“ˆ æœ¬æ­¥å¥–åŠ±: {reward:+.2f}")
        
        if self.server:
            with self.server.data_lock:
                grid_data = self.server.grid_data
                if grid_data and grid_data.cells:
                    total_cells = len(grid_data.cells)
                    scanned_cells = sum(
                        1 for cell in grid_data.cells
                        if cell.entropy < self.reward_config.scan_entropy_threshold
                    )
                    scan_progress = (scanned_cells / total_cells) * 100
                    print(f"ğŸ—ºï¸  æ‰«æè¿›åº¦: {scanned_cells}/{total_cells} ({scan_progress:.1f}%)")
        
        if done:
            print(f"\n{'='*60}")
            print(f"âœ… Episode #{self.episode_count} å®Œæˆï¼å…± {self.step_count} æ­¥")
            print(f"{'='*60}")
            print(f"ğŸ”„ å³å°†è‡ªåŠ¨é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªEpisode...")
            print(f"{'='*60}\n")
        
        # é¢å¤–ä¿¡æ¯
        info = {
            'weights': weights,
            'scanned_cells': self.prev_scanned_cells
        }
        
        return next_state, reward, done, info
    
    def _get_state(self):
        """è·å–å½“å‰çŠ¶æ€ï¼ˆ18ç»´ï¼‰"""
        if not self.server:
            # å¦‚æœæ²¡æœ‰serverï¼Œè¿”å›éšæœºçŠ¶æ€ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            return np.random.randn(18).astype(np.float32)
        
        try:
            with self.server.data_lock:
                runtime_data = self.server.unity_runtime_data[self.drone_name]
                grid_data = self.server.grid_data
                
                # 1. ä½ç½® (3)
                pos = runtime_data.position
                position = [pos.x, pos.y, pos.z]
                
                # 2. é€Ÿåº¦ (3)
                vel = runtime_data.finalMoveDir
                velocity = [
                    vel.x * self.server.config_data.moveSpeed,
                    vel.y * self.server.config_data.moveSpeed,
                    vel.z * self.server.config_data.moveSpeed
                ]
                
                # 3. æ–¹å‘ (3)
                fwd = runtime_data.forward
                direction = [fwd.x, fwd.y, fwd.z]
                
                # 4. é™„è¿‘ç†µå€¼ (3)
                entropy_info = self._get_entropy_info(grid_data, pos)
                
                # 5. Leaderç›¸å¯¹ä½ç½® (3)
                if runtime_data.leader_position:
                    leader_rel = [
                        runtime_data.leader_position.x - pos.x,
                        runtime_data.leader_position.y - pos.y,
                        runtime_data.leader_position.z - pos.z
                    ]
                else:
                    leader_rel = [0.0, 0.0, 0.0]
                
                # 6. æ‰«æè¿›åº¦ (3)
                scan_info = self._get_scan_info(grid_data)
                
                # ç»„åˆçŠ¶æ€
                state = position + velocity + direction + entropy_info + leader_rel + scan_info
                
                return np.array(state, dtype=np.float32)
                
        except Exception as e:
            print(f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")
            return np.zeros(18, dtype=np.float32)
    
    def _calculate_reward(self, action: np.ndarray) -> float:
        """è®¡ç®—å¥–åŠ±ï¼ˆå°½é‡ä¸å®ä½“å¥–åŠ±ç»“æ„ä¸€è‡´ï¼‰"""
        if not self.server:
            return 0.0

        reward = 0.0

        try:
            with self.server.data_lock:
                runtime_data = self.server.unity_runtime_data[self.drone_name]
                grid_data = self.server.grid_data

            # 1. é€Ÿåº¦å¥–åŠ±ä¸è¶…é€Ÿæƒ©ç½š
            vel = runtime_data.finalMoveDir
            current_velocity = np.array(
                [vel.x, vel.y, vel.z],
                dtype=np.float32
            ) * float(self.server.config_data.moveSpeed)
            speed = float(np.linalg.norm(current_velocity))
            reward += self.reward_config.speed_reward * speed
            if speed > self.reward_config.speed_penalty_threshold:
                reward -= self.reward_config.speed_penalty

            # 2. åŠ é€Ÿåº¦æƒ©ç½šï¼ˆé€Ÿåº¦å˜åŒ–è¿‘ä¼¼ï¼‰
            if self.step_duration > 0:
                accel_mag = float(np.linalg.norm(current_velocity - self.prev_velocity) / self.step_duration)
            else:
                accel_mag = float(np.linalg.norm(current_velocity - self.prev_velocity))
            reward -= self.reward_config.accel_penalty * accel_mag

            # 3. è§’é€Ÿåº¦æƒ©ç½šï¼ˆæ–¹å‘å˜åŒ–è¿‘ä¼¼ï¼‰
            fwd = runtime_data.forward
            current_direction = np.array([fwd.x, fwd.y, fwd.z], dtype=np.float32)
            current_norm = np.linalg.norm(current_direction)
            prev_norm = np.linalg.norm(self.prev_direction)
            if current_norm > 1e-6 and prev_norm > 1e-6:
                dot = float(np.clip(np.dot(current_direction, self.prev_direction) / (current_norm * prev_norm), -1.0, 1.0))
                angle = float(np.arccos(dot))
                angular_rate = angle / self.step_duration if self.step_duration > 0 else angle
                reward -= self.reward_config.angular_rate_penalty * angular_rate

            # 4. æ‰«æå¥–åŠ±
            current_scanned = 0
            if grid_data and grid_data.cells:
                current_scanned = sum(
                    1 for cell in grid_data.cells
                    if cell.entropy < self.reward_config.scan_entropy_threshold
                )
            new_scanned = current_scanned - self.prev_scanned_cells
            if new_scanned > 0:
                reward += self.reward_config.scan_reward * new_scanned
            self.prev_scanned_cells = current_scanned

            # 5. è¶Šç•Œæƒ©ç½šï¼ˆLeaderèŒƒå›´ï¼‰
            if runtime_data.leader_position and runtime_data.leader_scan_radius > 0:
                dist_to_leader = (runtime_data.position - runtime_data.leader_position).magnitude()
                leader_radius = runtime_data.leader_scan_radius + self.reward_config.leader_range_buffer
                if dist_to_leader > leader_radius:
                    reward -= self.reward_config.out_of_range_penalty

            # 6. åŠ¨ä½œå˜åŒ–ä¸å¹…åº¦æƒ©ç½š
            action_delta = float(np.linalg.norm(action - self.last_action))
            reward -= self.reward_config.action_change_penalty * action_delta
            reward -= self.reward_config.action_magnitude_penalty * float(np.linalg.norm(action))

            # 7. ç”µé‡å¥–åŠ±æœºåˆ¶
            current_voltage = self.server.get_battery_voltage(self.drone_name)
            if self.reward_config.battery_optimal_min <= current_voltage <= self.reward_config.battery_optimal_max:
                reward += self.reward_config.battery_optimal_reward
                print(f"ğŸ”‹ ç”µé‡å¥–åŠ±: +{self.reward_config.battery_optimal_reward:.2f} (ç”µé‡{current_voltage:.2f}Våœ¨æœ€ä¼˜èŒƒå›´)")
            elif current_voltage < self.reward_config.battery_low_threshold:
                reward -= self.reward_config.battery_low_penalty
                print(f"ğŸ”‹ ç”µé‡æƒ©ç½š: -{self.reward_config.battery_low_penalty:.2f} (ç”µé‡{current_voltage:.2f}Vè¿‡ä½)")

            # æ›´æ–°å†å²é€Ÿåº¦/æ–¹å‘
            self.prev_velocity = current_velocity
            if current_norm > 1e-6:
                self.prev_direction = current_direction

        except Exception as e:
            print(f"[é”™è¯¯] è®¡ç®—å¥–åŠ±å¤±è´¥: {str(e)}")

        return reward
    
    def _get_entropy_info(self, grid_data, position):
        """è·å–é™„è¿‘ç†µå€¼ä¿¡æ¯"""
        if not grid_data or not grid_data.cells:
            return [0.0, 0.0, 0.0]
        
        # æ‰¾é™„è¿‘10ç±³å†…çš„å•å…ƒæ ¼
        nearby_cells = [
            cell for cell in grid_data.cells[:100]
            if (cell.center - position).magnitude() < 10.0
        ]
        
        if not nearby_cells:
            return [0.0, 0.0, 0.0]
        
        entropies = [cell.entropy for cell in nearby_cells]
        return [
            float(np.mean(entropies)),
            float(np.max(entropies)),
            float(np.std(entropies))
        ]
    
    def _get_scan_info(self, grid_data):
        """è·å–æ‰«æè¿›åº¦"""
        if not grid_data or not grid_data.cells:
            return [0.0, 0.0, 0.0]
        
        total = len(grid_data.cells)
        scanned = sum(
            1 for cell in grid_data.cells
            if cell.entropy < self.reward_config.scan_entropy_threshold
        )
        
        return [
            scanned / max(total, 1),
            float(scanned),
            float(total - scanned)
        ]
    
    def _count_scanned_cells(self):
        """ç»Ÿè®¡å·²æ‰«æå•å…ƒæ ¼ï¼ˆä¸åŠ é”ç‰ˆæœ¬ï¼Œç”±è°ƒç”¨è€…åŠ é”ï¼‰"""
        if not self.server or not self.server.grid_data:
            return 0
        
        try:
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡ŒåŠ é”ï¼Œé¿å…åµŒå¥—é”
            # è°ƒç”¨è€…åº”è¯¥å·²ç»æŒæœ‰data_lock
            return sum(
                1 for cell in self.server.grid_data.cells
                if cell.entropy < self.reward_config.scan_entropy_threshold
            )
        except:
            return 0

    def set_initial_action(self, weights: np.ndarray) -> None:
        """è®¾ç½®åˆå§‹åŠ¨ä½œæƒé‡ï¼Œç”¨äºä¸å®ä½“è®­ç»ƒå¯¹é½å®‰å…¨è£å‰ª"""
        if weights is None:
            return
        weights = np.array(weights, dtype=np.float32)
        if weights.shape[0] != 5:
            return
        weights = np.clip(weights, self.reward_config.weight_min, self.reward_config.weight_max)
        self.last_action = weights.copy()
        self._has_initial_action = True


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•SimpleWeightEnv...")
    
    # æµ‹è¯•ä¸¤ç§æ¨¡å¼
    print("\n[æ¨¡å¼A] æ ‡å‡†episodeè®­ç»ƒ:")
    env_a = SimpleWeightEnv(server=None, drone_name="UAV1", reset_unity=True)
    print(f"  è§‚å¯Ÿç©ºé—´: {env_a.observation_space.shape}")
    print(f"  åŠ¨ä½œç©ºé—´: {env_a.action_space.shape}")
    
    print("\n[æ¨¡å¼B] è¿ç»­å­¦ä¹ :")
    env_b = SimpleWeightEnv(server=None, drone_name="UAV1", reset_unity=False)
    print(f"  è§‚å¯Ÿç©ºé—´: {env_b.observation_space.shape}")
    print(f"  åŠ¨ä½œç©ºé—´: {env_b.action_space.shape}")
    
    print("\n[OK] ä¸¤ç§æ¨¡å¼éƒ½å¯ç”¨ï¼")