{
  "movement": {
    "step_size": 2.0,
    "max_steps": 150,
    "ll_steps_per_hl": 10,
    "ll_step_duration": 0.5
  },
  "rewards": {
    "exploration": 15.0,
    "collision": -100.0,
    "out_of_range": -50.0,
    "goal_reached": 100.0,
    "step_penalty": -2.0,
    "success": 500.0,
    "ll_efficiency_weight": 0.1,
    "entropy_reduction_weight": 2.0,
    "height_penalty": -5.0,
    "optimal_height_bonus": 2.0
  },
  "thresholds": {
    "collision_distance": 3.0,
    "scanned_entropy": 10.0,
    "success_scan_ratio": 0.98,
    "min_scan_height": 2.0,
    "max_scan_height": 15.0,
    "optimal_scan_height": 8.0,
    "high_entropy_threshold": 40.0
  },
  "model": {
    "policy": "MlpPolicy",
    "net_arch": [256, 256, 128]
  },
  "training": {
    "total_timesteps": 80000,
    "learning_rate": 3e-4,
    "buffer_size": 15000,
    "learning_starts": 2000,
    "batch_size": 64,
    "tau": 1.0,
    "gamma": 0.99,
    "target_update_interval": 1000,
    "exploration_fraction": 0.3,
    "exploration_initial_eps": 1.0,
    "exploration_final_eps": 0.05
  }
}
