"""
æ”¹è¿›ç‰ˆAirSimç¯å¢ƒè®­ç»ƒè„šæœ¬
è§£å†³Unityå¡æ­»é—®é¢˜
æ”¯æŒCtrl+Cå¼ºåˆ¶é€€å‡º
"""
import os
import sys
import time
import signal
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å…¨å±€æ ‡å¿—ï¼Œç”¨äºCtrl+Cå¤„ç†
training_interrupted = False

def signal_handler(sig, frame):
    """å¤„ç†Ctrl+Cä¿¡å·"""
    global training_interrupted
    if not training_interrupted:
        print("\n\n" + "=" * 60)
        print("[ä¸­æ–­] æ£€æµ‹åˆ°Ctrl+Cï¼Œæ­£åœ¨åœæ­¢è®­ç»ƒ...")
        print("=" * 60)
        training_interrupted = True
    else:
        print("\n[å¼ºåˆ¶é€€å‡º] å†æ¬¡æŒ‰Ctrl+Cå°†å¼ºåˆ¶é€€å‡ºç¨‹åº")
        sys.exit(1)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

print("=" * 60)
print("DQNè®­ç»ƒ - æ”¹è¿›ç‰ˆï¼ˆé˜²æ­¢Unityå¡æ­»ï¼‰")
print("=" * 60)

# æ£€æŸ¥ä¾èµ–
print("\næ£€æŸ¥ä¾èµ–...")
try:
    import torch
    from stable_baselines3 import DDPG
    from stable_baselines3.common.noise import NormalActionNoise
    from stable_baselines3.common.callbacks import BaseCallback
    print("[OK] ä¾èµ–æ£€æŸ¥é€šè¿‡")
except ImportError as e:
    print(f"[é”™è¯¯] ç¼ºå°‘ä¾èµ–: {e}")
    input("æŒ‰Enteré€€å‡º...")
    sys.exit(1)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from simple_weight_env import SimpleWeightEnv
from training_visualizer import TrainingVisualizer

# å¯¼å…¥AlgorithmServer
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from AlgorithmServer import MultiDroneAlgorithmServer


class ImprovedTrainingCallback(BaseCallback):
    """æ”¹è¿›çš„è®­ç»ƒå›è°ƒï¼Œçªå‡ºæ˜¾ç¤ºæ¨¡å‹å’Œå¥–åŠ±ï¼Œå¹¶æ›´æ–°å¯è§†åŒ–"""
    
    def __init__(self, total_timesteps, check_freq=1000, save_path='./models/', 
                 training_visualizer=None, verbose=1):
        super(ImprovedTrainingCallback, self).__init__(verbose)
        self.total_timesteps = total_timesteps
        self.check_freq = check_freq
        self.save_path = save_path
        self.training_visualizer = training_visualizer  # è®­ç»ƒå¯è§†åŒ–å™¨
        self.best_mean_reward = -np.inf
        self.last_print_step = 0
        self.print_interval = max(total_timesteps // 10, 100)  # åªæ˜¾ç¤º10æ¬¡
        self.episode_count = 0
        self.episode_rewards = []
        
        os.makedirs(save_path, exist_ok=True)
        
    def _on_step(self) -> bool:
        # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
        global training_interrupted
        if training_interrupted:
            print("\n[ä¸­æ–­] åœæ­¢è®­ç»ƒ...")
            return False  # è¿”å›Falseåœæ­¢è®­ç»ƒ
        
        # è®°å½•episodeå¥–åŠ±
        if len(self.model.ep_info_buffer) > 0 and len(self.model.ep_info_buffer) > self.episode_count:
            ep_reward = self.model.ep_info_buffer[-1]['r']
            ep_length = self.model.ep_info_buffer[-1]['l']
            self.episode_rewards.append(ep_reward)
            self.episode_count = len(self.model.ep_info_buffer)
            
            # æ›´æ–°è®­ç»ƒå¯è§†åŒ–
            if self.training_visualizer:
                self.training_visualizer.update_training_stats(
                    episode_reward=ep_reward,
                    episode_length=ep_length,
                    is_episode_done=True
                )
            
            print(f"\n{'â•”'+'â•'*58+'â•—'}")
            print(f"â•‘  ğŸ‰ Episode #{self.episode_count} å®Œæˆï¼{' '*(45-len(str(self.episode_count)))}â•‘")
            print(f"{'â• '+'â•'*58+'â•£'}")
            print(f"â•‘  ğŸ“ˆ æœ¬æ¬¡å¥–åŠ±: {ep_reward:+8.2f}{' '*40}â•‘")
            print(f"â•‘  ğŸ“ Episodeé•¿åº¦: {ep_length:4.0f} æ­¥{' '*36}â•‘")
            
            if len(self.episode_rewards) > 1:
                avg_reward = np.mean(self.episode_rewards)
                best_reward = max(self.episode_rewards)
                worst_reward = min(self.episode_rewards)
                print(f"â•‘{' '*58}â•‘")
                print(f"â•‘  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:{' '*43}â•‘")
                print(f"â•‘    â€¢ å¹³å‡å¥–åŠ±: {avg_reward:+8.2f}{' '*35}â•‘")
                print(f"â•‘    â€¢ æœ€ä½³å¥–åŠ±: {best_reward:+8.2f}{' '*35}â•‘")
                print(f"â•‘    â€¢ æœ€å·®å¥–åŠ±: {worst_reward:+8.2f}{' '*35}â•‘")
                
                # å¥–åŠ±è¶‹åŠ¿
                if len(self.episode_rewards) >= 3:
                    recent_avg = np.mean(self.episode_rewards[-3:])
                    trend = "ğŸ“ˆ ä¸Šå‡" if recent_avg > avg_reward else "ğŸ“‰ ä¸‹é™"
                    print(f"â•‘    â€¢ æœ€è¿‘è¶‹åŠ¿: {trend}{' '*35}â•‘")
            
            print(f"â•‘{' '*58}â•‘")
            remaining_steps = self.total_timesteps - self.num_timesteps
            progress = self.num_timesteps / self.total_timesteps * 100
            print(f"â•‘  ğŸ¯ è®­ç»ƒè¿›åº¦: {self.num_timesteps}/{self.total_timesteps} ({progress:.1f}%){' '*(24-len(str(self.total_timesteps))*2-len(f'{progress:.1f}'))}â•‘")
            print(f"â•‘  â³ å‰©ä½™æ­¥æ•°: {remaining_steps}{' '*(43-len(str(remaining_steps)))}â•‘")
            print(f"{'â•š'+'â•'*58+'â•'}\n")
            
            # å¦‚æœè®­ç»ƒè¿˜æ²¡ç»“æŸï¼Œæç¤ºå³å°†å¼€å§‹ä¸‹ä¸€ä¸ªEpisode
            if self.num_timesteps < self.total_timesteps:
                print(f"{'â”€'*60}")
                print(f"ğŸ”„ å‡†å¤‡ä¸‹ä¸€ä¸ªEpisodeï¼ˆ#{self.episode_count + 1}ï¼‰...")
                print(f"   ç¯å¢ƒå°†è‡ªåŠ¨é‡ç½®...")
                print(f"{'â”€'*60}\n")
        
        # å‡å°‘æ‰“å°é¢‘ç‡ï¼Œé¿å…é˜»å¡
        if self.num_timesteps - self.last_print_step >= self.print_interval:
            if len(self.model.ep_info_buffer) > 0:
                mean_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])
            else:
                mean_reward = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if mean_reward > self.best_mean_reward and mean_reward > 0:
                self.best_mean_reward = mean_reward
                model_path = os.path.join(self.save_path, 'best_model')
                self.model.save(model_path)
                print(f"\nğŸ† æ–°æœ€ä½³æ¨¡å‹ï¼å¥–åŠ±: {mean_reward:.2f}")
                print(f"ğŸ’¾ å·²ä¿å­˜: {model_path}.zip\n")
            
            self.last_print_step = self.num_timesteps
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps % self.check_freq == 0 and self.num_timesteps > 0:
            checkpoint_path = os.path.join(self.save_path, f'checkpoint_{self.num_timesteps}')
            self.model.save(checkpoint_path)
            print(f"ğŸ’¾ æ£€æŸ¥ç‚¹: checkpoint_{self.num_timesteps}.zip")
        
        return True  # ç»§ç»­è®­ç»ƒ


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    
    # ==================== è®­ç»ƒå‚æ•°é…ç½® ====================
    DRONE_NAMES = ["UAV1", "UAV2", "UAV3"]  # ä½¿ç”¨4å°æ— äººæœºååŒè®­ç»ƒ
    TOTAL_TIMESTEPS = 100            # æ€»è®­ç»ƒæ­¥æ•°ï¼ˆå¿«é€Ÿè®­ç»ƒï¼‰
    STEP_DURATION = 20.0             # æ¯æ­¥é£è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰ æé«˜é£è¡Œæ—¶é•¿
    CHECKPOINT_FREQ = 1000           # æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡
    ENABLE_VISUALIZATION = True      # æ˜¯å¦å¯ç”¨å¯è§†åŒ–ï¼ˆè®­ç»ƒä¸“ç”¨å¯è§†åŒ–ï¼‰
    # =====================================================
    
    # å…¨å±€å˜é‡ï¼Œç”¨äºæ¸…ç†
    server = None
    training_visualizer = None
    
    print("\n" + "=" * 60)
    print("ğŸš€ DQNæƒé‡è®­ç»ƒ - å¤šæ— äººæœºååŒæ¨¡å¼")
    print("=" * 60)
    print(f"ğŸš æ— äººæœºæ•°é‡: {len(DRONE_NAMES)} å° ({', '.join(DRONE_NAMES)})")
    print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {TOTAL_TIMESTEPS} æ­¥")
    print(f"â±ï¸  æ¯æ­¥æ—¶é•¿: {STEP_DURATION} ç§’")
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹: æ¯ {CHECKPOINT_FREQ} æ­¥ä¿å­˜ä¸€æ¬¡")
    print(f"ğŸ‘ï¸  å¯è§†åŒ–: {'å¯ç”¨' if ENABLE_VISUALIZATION else 'ç¦ç”¨'}")
    print(f"ğŸ“ˆ é¢„è®¡episodeæ•°: ~{TOTAL_TIMESTEPS // 50}")
    print("=" * 60)
    print(f"\nğŸ’¡ è¯´æ˜: ä½¿ç”¨{len(DRONE_NAMES)}å°æ— äººæœºååŒè®­ç»ƒ")
    print(f"   - ä¸»è®­ç»ƒæ— äººæœº: {DRONE_NAMES[0]} (ç”¨äºDQNå­¦ä¹ )")
    print(f"   - ååŒæ— äººæœº: {', '.join(DRONE_NAMES[1:])} (æä¾›ç¯å¢ƒäº¤äº’)")
    print(f"   - å­¦åˆ°çš„æƒé‡ç­–ç•¥å°†é€‚ç”¨äºæ‰€æœ‰æ— äººæœº")
    print("\n[é‡è¦] è¯·ç¡®ä¿Unity AirSimä»¿çœŸå·²ç»è¿è¡Œï¼")
    
    confirm = input("Unityå·²è¿è¡Œï¼Ÿ(Y/N): ").strip().upper()
    if confirm != 'Y':
        print("è¯·å…ˆå¯åŠ¨Unity")
        return
    
    try:
        print("\n[1/5] å¯åŠ¨AlgorithmServer...")
        
        # åˆ›å»ºæœåŠ¡å™¨ï¼ˆè®­ç»ƒæ¨¡å¼ä¸ä½¿ç”¨å­¦ä¹ çš„æƒé‡ï¼Œç¦ç”¨AlgorithmServerè‡ªå¸¦çš„å¯è§†åŒ–ï¼‰
        server = MultiDroneAlgorithmServer(
            drone_names=DRONE_NAMES,
            use_learned_weights=False,
            model_path=None,  # è®­ç»ƒæ¨¡å¼ä¸éœ€è¦åŠ è½½æ¨¡å‹
            enable_visualization=False  # ç¦ç”¨AlgorithmServerçš„å¯è§†åŒ–ï¼Œä½¿ç”¨è®­ç»ƒä¸“ç”¨å¯è§†åŒ–
        )
        
        print(f"âœ… æœåŠ¡å™¨åˆ›å»ºæˆåŠŸ")
        print(f"  æ— äººæœºé…ç½®: {', '.join(DRONE_NAMES)}")
        print(f"  ä½¿ç”¨è®­ç»ƒä¸“ç”¨å¯è§†åŒ–: {'æ˜¯' if ENABLE_VISUALIZATION else 'å¦'}")
    
        # å¯åŠ¨æœåŠ¡å™¨
        if not server.start():
            print("[é”™è¯¯] AlgorithmServerå¯åŠ¨å¤±è´¥")
            return
        
        print("[OK] AlgorithmServerå·²è¿æ¥")
        
        # å¯åŠ¨æ— äººæœºå’Œç®—æ³•çº¿ç¨‹
        print("\n[2/5] å¯åŠ¨æ— äººæœºä»»åŠ¡...")
        print("[é‡è¦] è®­ç»ƒæ¨¡å¼ï¼šå¯åŠ¨ç®—æ³•çº¿ç¨‹ï¼Œè®­ç»ƒç¯å¢ƒåŠ¨æ€æ”¹å˜æƒé‡")
        
        # è°ƒç”¨start_mission()å¯åŠ¨å®Œæ•´æµç¨‹
        if not server.start_mission():
            print("[é”™è¯¯] ä»»åŠ¡å¯åŠ¨å¤±è´¥")
            server.stop()
            return
        
        print("[OK] æ— äººæœºå·²èµ·é£ï¼Œç®—æ³•çº¿ç¨‹è¿è¡Œä¸­")
        
        # ç­‰å¾…ç³»ç»Ÿç¨³å®š
        print("\n[3/5] ç­‰å¾…ç³»ç»Ÿç¨³å®š...")
        time.sleep(5)
        
        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        print("\n[4/5] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
        
        env = SimpleWeightEnv(
            server=server,
            drone_name=DRONE_NAMES[0],  # ä½¿ç”¨ç¬¬ä¸€å°æ— äººæœºè¿›è¡ŒDQNè®­ç»ƒ
            reset_unity=True,          # æ ‡å‡†episodeè®­ç»ƒ
            step_duration=STEP_DURATION  # ä½¿ç”¨é…ç½®çš„é£è¡Œæ—¶é•¿
        )
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"  ğŸ“‹ æ¨¡å¼: å¤šæ— äººæœºååŒè®­ç»ƒ")
        print(f"  ğŸ“ è®­ç»ƒæ— äººæœº: {DRONE_NAMES[0]}")
        print(f"  ğŸ¤ ååŒæ— äººæœº: {', '.join(DRONE_NAMES[1:]) if len(DRONE_NAMES) > 1 else 'æ— '}")
        print(f"  â±ï¸  æ¯æ­¥æ—¶é•¿: {STEP_DURATION}ç§’")
        print(f"  ğŸ¯ æ¯ä¸ªepisode: {env.reward_config.max_steps}æ­¥ = {env.reward_config.max_steps * STEP_DURATION / 60:.1f}åˆ†é’Ÿ")
        print(f"  ğŸ’¡ é¢„è®¡æ€»è®­ç»ƒæ—¶é•¿: {TOTAL_TIMESTEPS * STEP_DURATION / 60:.1f}åˆ†é’Ÿ")
        
        # åˆ›å»ºå¹¶å¯åŠ¨è®­ç»ƒä¸“ç”¨å¯è§†åŒ–
        if ENABLE_VISUALIZATION:
            print("\n[4.5/5] å¯åŠ¨è®­ç»ƒä¸“ç”¨å¯è§†åŒ–...")
            try:
                training_visualizer = TrainingVisualizer(server=server, env=env)
                if training_visualizer.start_visualization():
                    print("âœ… è®­ç»ƒå¯è§†åŒ–å·²å¯åŠ¨")
                    print("ğŸ’¡ å¯è§†åŒ–çª—å£åº”è¯¥ä¼šå¼¹å‡ºï¼Œæ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡å’Œç¯å¢ƒçŠ¶æ€")
                    print("ğŸ’¡ æŒ‰ESCé”®å¯å…³é—­å¯è§†åŒ–çª—å£ï¼ˆä¸å½±å“è®­ç»ƒï¼‰")
                else:
                    print("âš ï¸  è®­ç»ƒå¯è§†åŒ–å¯åŠ¨å¤±è´¥ï¼Œä½†è®­ç»ƒå°†ç»§ç»­")
            except Exception as e:
                print(f"âš ï¸  è®­ç»ƒå¯è§†åŒ–åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                print("ğŸ’¡ è®­ç»ƒå°†ç»§ç»­ï¼Œä½†ä¸æ˜¾ç¤ºå¯è§†åŒ–")
                training_visualizer = None
        
        # åˆ›å»ºDDPGæ¨¡å‹
        print("\n[5/5] åˆ›å»ºDDPGæ¨¡å‹...")
        
        n_actions = env.action_space.shape[0]
        action_noise = NormalActionNoise(
            mean=np.zeros(n_actions),
            sigma=0.15 * np.ones(n_actions)  # é€‚åº¦å™ªå£°
        )
        
        model = DDPG(
            "MlpPolicy",
            env,
            action_noise=action_noise,
            learning_rate=1e-4,
            buffer_size=5000,        # å°ç¼“å†²åŒºï¼ˆå¿«é€Ÿè®­ç»ƒï¼‰
            learning_starts=200,     # å°½æ—©å¼€å§‹å­¦ä¹ 
            batch_size=64,
            tau=0.005,
            gamma=0.99,
            train_freq=(1, "episode"),
            gradient_steps=-1,
            verbose=0,
            device='cpu'
        )
        
        print("âœ… DDPGæ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # å¼€å§‹è®­ç»ƒ
        print("\n" + "=" * 60)
        print("ğŸ¯ å¼€å§‹è®­ç»ƒ")
        print("=" * 60)
        print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {TOTAL_TIMESTEPS}")
        print(f"â¸ï¸  æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢")
        print("=" * 60 + "\n")
        
        model_dir = os.path.join(os.path.dirname(__file__), 'models')
        os.makedirs(model_dir, exist_ok=True)
        
        training_callback = ImprovedTrainingCallback(
            total_timesteps=TOTAL_TIMESTEPS,
            check_freq=CHECKPOINT_FREQ,
            save_path=model_dir,
            training_visualizer=training_visualizer,  # ä¼ å…¥å¯è§†åŒ–å™¨
            verbose=1
        )
        
        model.learn(
            total_timesteps=TOTAL_TIMESTEPS,
            log_interval=None,
            callback=training_callback
        )
        
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        final_model_path = os.path.join(model_dir, 'weight_predictor_airsim')
        model.save(final_model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {final_model_path}.zip")
        
        # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
        print("\n" + "=" * 60)
        print("ğŸ“Š è®­ç»ƒç»Ÿè®¡")
        print("=" * 60)
        if hasattr(training_callback, 'episode_rewards') and training_callback.episode_rewards:
            print(f"å®Œæˆepisodeæ•°: {len(training_callback.episode_rewards)}")
            print(f"æ€»å¥–åŠ±: {sum(training_callback.episode_rewards):.2f}")
            print(f"å¹³å‡å¥–åŠ±: {np.mean(training_callback.episode_rewards):.2f}")
            print(f"æœ€ä½³å¥–åŠ±: {max(training_callback.episode_rewards):.2f}")
            print(f"æœ€å·®å¥–åŠ±: {min(training_callback.episode_rewards):.2f}")
        print("=" * 60)
        
        print("\nğŸ“¦ ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶:")
        print(f"  ğŸ† æœ€ä½³æ¨¡å‹: models/best_model.zip")
        print(f"  ğŸ“„ æœ€ç»ˆæ¨¡å‹: models/weight_predictor_airsim.zip")
        if CHECKPOINT_FREQ > 0:
            print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹: models/checkpoint_*.zip")
        
        print("\nğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("  1ï¸âƒ£  æµ‹è¯•æ¨¡å‹: python test_trained_model.py")
        print("  2ï¸âƒ£  ä½¿ç”¨æ¨¡å‹: python ../AlgorithmServer.py --use-learned-weights")
        print("=" * 60)
        
    except KeyboardInterrupt:
        print("\n\n" + "=" * 60)
        print("[ä¸­æ–­] æ­£åœ¨åœæ­¢è®­ç»ƒ...")
        print("=" * 60)
        print("\nè¯·ç¨å€™ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
        
    except Exception as e:
        print(f"\n\n[é”™è¯¯] è®­ç»ƒå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ç¡®ä¿æ¸…ç†èµ„æº
        
        # åœæ­¢å¯è§†åŒ–
        if training_visualizer:
            print("\nåœæ­¢è®­ç»ƒå¯è§†åŒ–...")
            try:
                training_visualizer.stop_visualization()
                print("[OK] è®­ç»ƒå¯è§†åŒ–å·²åœæ­¢")
            except Exception as e:
                print(f"[è­¦å‘Š] åœæ­¢å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        
        if server:
            print("\nåœæ­¢AlgorithmServer...")
            try:
                # é™è½æ— äººæœº
                for drone_name in DRONE_NAMES:
                    try:
                        print(f"  é™è½ {drone_name}...")
                        server.drone_controller.land(drone_name)
                    except:
                        pass
                
                # åœæ­¢æœåŠ¡å™¨ï¼ˆç”±äºæ²¡å¯åŠ¨ç®—æ³•çº¿ç¨‹ï¼Œè¿™é‡Œåªæ˜¯æ–­å¼€è¿æ¥ï¼‰
                server.unity_socket.stop()
                print("[OK] AlgorithmServerå·²åœæ­¢")
            except Exception as e:
                print(f"[è­¦å‘Š] æ¸…ç†èµ„æºæ—¶å‡ºç°é”™è¯¯: {e}")
        
        print("\nè®­ç»ƒå·²ç»“æŸ")
        print("æŒ‰Enteré”®é€€å‡º...")
        try:
            input()
        except:
            pass


if __name__ == "__main__":
    main()

