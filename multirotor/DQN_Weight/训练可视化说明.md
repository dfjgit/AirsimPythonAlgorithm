# 🎨 训练可视化说明

## 📋 概述

训练专用可视化模块 (`training_visualizer.py`) 是专门为DQN训练设计的实时监控工具，与AlgorithmServer的常规可视化相比，它提供了训练特有的统计信息和实时反馈。

---

## 🆚 两种可视化的区别

### SimpleVisualizer（常规可视化）
- **用途**: 正常运行时的环境监控
- **显示内容**:
  - 环境状态（网格、熵值）
  - 无人机位置和方向
  - Leader位置
  - 系统状态
  - APF权重（DQN模式下显示预测权重）
  - 重置按钮
- **使用场景**: 
  - 运行AlgorithmServer时
  - 测试模型时
  - 实际部署时

### TrainingVisualizer（训练可视化）⭐
- **用途**: DQN训练过程监控
- **显示内容**:
  - ✅ **训练统计面板** (右上角)
    - Episode数量
    - 总训练步数
    - 当前Episode进度
    - 平均/最佳/最差奖励
  - ✅ **奖励曲线图** (右下角)
    - 最近50个Episode的奖励趋势
    - 平均奖励线
    - 数据点标记
  - ✅ **当前权重面板** (左下角)
    - 实时显示训练环境设置的权重
    - 权重值可视化条
  - ✅ **环境状态面板** (左上角)
    - 网格统计
    - 扫描进度
    - 训练模式提示
  - ✅ **环境视图** (中央)
    - 网格熵值分布
    - 无人机位置
    - Leader位置
- **使用场景**: 
  - 训练新模型时
  - 调试训练参数时
  - 监控训练进度时

---

## 🎯 训练可视化窗口布局

```
┌─────────────────────────────────────────────────────────────┐
│  [环境状态]          [环境视图 - 中央]          [训练统计]  │
│  🌍 网格统计                                      🎯 Episode │
│  扫描进度            网格、无人机、Leader          总步数   │
│  训练模式                                          当前进度 │
│                                                    奖励统计 │
│                                                             │
│  [当前权重]                                      [奖励曲线]│
│  ⚙️ α1-α5                                         📈 趋势图 │
│  实时权重值                                       最近50个  │
│  可视化条                                         平均线   │
└─────────────────────────────────────────────────────────────┘
```

---

## 🚀 使用方法

### 启动训练（自动启用可视化）

```bash
cd AirsimAlgorithmPython/multirotor/DQN_Weight
python train_with_airsim_improved.py
```

**预期输出**：
```
[1/5] 启动AlgorithmServer...
✅ 服务器创建成功
  使用训练专用可视化: 是

[2/5] 启动无人机任务...
✅ 无人机已起飞，算法线程运行中

[3/5] 等待系统稳定...

[4/5] 创建训练环境...
✅ 环境创建成功
  📋 模式: 标准episode训练
  ⏱️  每步时长: 10.0秒

[4.5/5] 启动训练专用可视化...
============================================================
✅ 训练可视化窗口已创建
💡 按ESC键关闭可视化窗口
============================================================
✅ 训练可视化已启动
💡 可视化窗口应该会弹出，显示训练统计和环境状态
💡 按ESC键可关闭可视化窗口（不影响训练）

[5/5] 创建DDPG模型...
✅ DDPG模型创建成功

🎯 开始训练
```

### 控制可视化

- **关闭可视化**: 按 `ESC` 键（训练继续）
- **停止训练**: 按 `Ctrl+C`（训练和可视化都停止）

---

## 📊 可视化面板详解

### 1. 训练统计面板（右上角）

显示内容：
```
🎯 训练状态
━━━━━━━━━━━━━━━━━━━━━━━━
Episode: 15
总步数: 750
当前Episode步数: 23
当前Episode奖励: 125.67

平均奖励: 118.45
最佳奖励: 156.23
最差奖励: 89.12
平均步长: 48.3

Episode最大步数: 50
```

**说明**：
- **Episode**: 完成的episode数量
- **总步数**: 所有episode的总步数
- **当前Episode**: 正在进行的episode的实时状态
- **平均奖励**: 所有已完成episode的平均奖励
- **最佳/最差**: 历史最佳和最差奖励
- **平均步长**: episode的平均长度

### 2. 奖励曲线图（右下角）

显示内容：
```
📈 奖励曲线（最近50个Episode）
━━━━━━━━━━━━━━━━━━━━━━━━
    │
156 │      ●
    │   ●    ●  ●
    │ ●  ●●   ●●  ●
118 │-------Avg-------  (平均线)
    │●
 89 │
    └─────────────────→
```

**说明**：
- 绿色曲线：奖励变化趋势
- 黄色点：每个episode的奖励
- 橙色线：平均奖励
- 自动缩放：Y轴根据奖励范围自动调整

### 3. 当前权重面板（左下角）

显示内容：
```
⚙️ 当前APF权重（训练中动态调整）
━━━━━━━━━━━━━━━━━━━━━━━━
α1 排斥:  2.34  ████████░░░░  2.34
α2 熵值:  3.12  ████████████░  3.12
α3 距离:  1.78  ██████░░░░░░  1.78
α4 Leader: 2.89  ███████████░  2.89
α5 方向:  1.95  ███████░░░░░  1.95
```

**说明**：
- **α1-α5**: 五个APF权重参数
- **数值**: 当前权重值（0.5-5.0范围）
- **可视化条**: 
  - 绿色: 低权重 (< 1.5)
  - 黄色: 中等权重 (1.5-3.0)
  - 红色: 高权重 (> 3.0)
- **实时更新**: 每步更新（训练环境动态设置）

### 4. 环境状态面板（左上角）

显示内容：
```
🌍 环境状态
━━━━━━━━━━━━━━━━━━━━━━━━
网格单元: 500
平均熵值: 45.6
已扫描: 312 (62.4%)

模式: DQN权重训练
```

**说明**：
- **网格单元**: 环境中的总单元数
- **平均熵值**: 当前环境的平均信息熵
- **已扫描**: 熵值<30的单元数和百分比
- **模式**: 训练模式提示

---

## 🔧 自定义配置

### 修改可视化参数

编辑 `training_visualizer.py`:

```python
# 窗口大小
self.SCREEN_WIDTH = 1400  # 宽度
self.SCREEN_HEIGHT = 900  # 高度

# 历史记录长度
self.episode_rewards: Deque[float] = deque(maxlen=100)  # 最多保存100个
self.reward_history: Deque[float] = deque(maxlen=50)    # 曲线显示50个

# 帧率
self.clock.tick(30)  # 30 FPS
```

### 禁用可视化（提升性能）

编辑 `train_with_airsim_improved.py`:

```python
ENABLE_VISUALIZATION = False  # 改为False
```

---

## 🐛 故障排除

### 问题1：可视化窗口没有弹出

**检查项**：
1. pygame是否安装：
   ```bash
   pip install pygame
   ```

2. 查看日志输出：
   ```
   [4.5/5] 启动训练专用可视化...
   ✅ 训练可视化已启动  ← 成功标志
   ```

3. 检查是否有错误提示：
   ```
   ⚠️  训练可视化初始化失败: ...
   ```

### 问题2：可视化窗口卡顿

**解决方法**：
1. 降低帧率：
   ```python
   self.clock.tick(20)  # 从30降到20
   ```

2. 减少历史数据量：
   ```python
   self.reward_history: Deque[float] = deque(maxlen=30)  # 从50降到30
   ```

3. 禁用可视化专注训练：
   ```python
   ENABLE_VISUALIZATION = False
   ```

### 问题3：统计数据不更新

**原因**：
- Episode可能还没有完成
- 训练刚开始，还没有足够数据

**验证**：
- 观察"当前Episode步数"是否在增加
- 等待第一个Episode完成

---

## 📈 使用建议

### 训练阶段

1. **初期训练**（前几个episode）：
   - ✅ 启用可视化观察环境行为
   - ✅ 检查奖励是否合理
   - ✅ 验证环境重置是否正常

2. **长时间训练**：
   - ⚠️ 可考虑禁用可视化提升性能
   - ✅ 或降低帧率减少开销

3. **调试阶段**：
   - ✅ 启用可视化观察细节
   - ✅ 关注权重变化和奖励趋势
   - ✅ 检查无人机是否卡住

### 性能优化

| 场景 | 帧率建议 | 历史长度 | 性能影响 |
|------|---------|---------|---------|
| 调试训练 | 30 FPS | 50 | 中等 |
| 正常训练 | 20 FPS | 30 | 较低 |
| 长时间训练 | 禁用 | N/A | 无 |

---

## 🎯 与常规可视化的对比

| 特性 | SimpleVisualizer | TrainingVisualizer |
|------|-----------------|-------------------|
| **使用场景** | 正常运行/测试 | DQN训练 |
| **环境显示** | ✅ | ✅ |
| **训练统计** | ❌ | ✅ Episode/步数/奖励 |
| **奖励曲线** | ❌ | ✅ 实时趋势图 |
| **权重显示** | ✅ 预测权重 | ✅ 训练权重 |
| **重置按钮** | ✅ | ❌ (自动重置) |
| **窗口大小** | 1200x800 | 1400x900 |

---

## 💡 最佳实践

1. **首次训练**：
   - ✅ 启用可视化
   - ✅ 观察前5-10个episode
   - ✅ 验证训练正常后可关闭

2. **持续监控**：
   - ✅ 定期查看奖励曲线
   - ✅ 关注权重变化趋势
   - ✅ 检查扫描效率

3. **问题诊断**：
   - ✅ 奖励不增长 → 观察权重是否合理
   - ✅ Episode太短 → 检查终止条件
   - ✅ 无人机卡住 → 观察环境状态

---

## 📝 总结

训练可视化器是专为DQN训练设计的强大工具，提供：
- 🎯 实时训练进度
- 📈 奖励趋势分析
- ⚙️ 权重变化监控
- 🌍 环境状态追踪

合理使用可大幅提升训练效率和调试能力！

