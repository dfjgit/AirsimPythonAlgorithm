"""
å¤šç®—æ³•è®­ç»ƒæ•°æ®ç»Ÿä¸€å¯¹æ¯”åˆ†æå·¥å…· (Unified Training Analyzer)
è®¾è®¡ç›®æ ‡ï¼šåŸºäº DataCollector äº§å‡ºçš„æ ‡å‡†åŒ– CSV æ•°æ®ï¼Œå®ç°è·¨ç®—æ³•ã€è·¨åœºæ™¯çš„è‡ªåŠ¨å¯¹æ¯”ã€‚
æ— éœ€ä¸ºæ–°ç®—æ³•ç¼–å†™æ–°ä»£ç ï¼Œåªéœ€åœ¨è®­ç»ƒè„šæœ¬ä¸­é€šè¿‡ set_experiment_meta è®¾ç½®æ ‡ç­¾å³å¯ã€‚
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("TrainingAnalyzer")

class UnifiedTrainingAnalyzer:
    # ç®—æ³• ID åˆ°å‹å¥½åç§°çš„æ˜ å°„
    ALGO_NAME_MAP = {
        'hrl_dqn_apf': 'åŒå±‚èåˆè®­ç»ƒ (HRL+APF)',
        'pure_dqn': 'çº¯ DQN ç§»åŠ¨æ§åˆ¶',
        'ddpg_apf': 'DDPG æƒé‡è‡ªé€‚åº” (APF)',
        'unknown': 'æœªæ ‡è®°ç®—æ³• (å†å²æ•°æ®)'
    }

    # æŒ‡æ ‡ ID åˆ°å‹å¥½åç§°çš„æ˜ å°„
    METRIC_NAME_MAP = {
        'reward': 'ç´¯è®¡å¥–åŠ±',
        'scan_efficiency': 'æ‰«ææ•ˆç‡ (Cell/Step)',
        'scan_ratio': 'æ‰«æå®Œæˆåº¦ (%)',
        'global_avg_entropy': 'å…¨å±€å¹³å‡ç†µ',
        'episode': 'è®­ç»ƒè½®æ¬¡ (Episode)',
        'elapsed_time': 'è¿è¡Œæ—¶é—´ (ç§’)'
    }

    def __init__(self, output_dir: str = "analysis_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.runs = []  # å­˜å‚¨æ‰€æœ‰åŠ è½½çš„å®éªŒæ•°æ®
        
        # ç»Ÿä¸€å­—ä½“é…ç½® (è§£å†³ä¸­æ–‡æ˜¾ç¤º)
        self._setup_plotting_style()

    def _setup_plotting_style(self):
        sns.set_theme(style="whitegrid")
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False

    def load_data(self, log_dirs: List[str]):
        """
        æ‰«æå¤šä¸ªç›®å½•ï¼ŒåŠ è½½æ‰€æœ‰ç¬¦åˆæ ¼å¼çš„ CSV æ–‡ä»¶ã€‚
        æ”¯æŒ training CSV å’Œ scan_data CSVã€‚
        """
        for d in log_dirs:
            p = Path(d)
            if not p.exists():
                logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {d}")
                continue
            
            # æŸ¥æ‰¾æ‰€æœ‰ CSV
            for csv_file in p.glob("*.csv"):
                try:
                    df = pd.read_csv(csv_file)
                    if df.empty: continue
                    
                    # å°è¯•ä»æ•°æ®ä¸­è¯†åˆ«ç®—æ³•æ ‡ç­¾
                    algo = df['algorithm_type'].iloc[0] if 'algorithm_type' in df.columns else None
                    
                    # å¦‚æœæ•°æ®ä¸­æ²¡æœ‰æ ‡ç­¾ï¼Œå°è¯•ä»è·¯å¾„çŒœæµ‹
                    if not algo or pd.isna(algo):
                        path_str = str(csv_file).lower()
                        if 'hrl' in path_str or 'hierarchical' in path_str:
                            algo = 'hrl_dqn_apf'
                        elif 'dqn' in path_str:
                            algo = 'pure_dqn'
                        elif 'ddpg' in path_str:
                            algo = 'ddpg_apf'
                        else:
                            algo = 'unknown'
                    
                    env = df['env_type'].iloc[0] if 'env_type' in df.columns else "unknown"
                    
                    # è®°å½•è¯¥æ¬¡è¿è¡Œçš„å…ƒæ•°æ®
                    run_info = {
                        'file': csv_file,
                        'name': csv_file.stem,
                        'algorithm': algo,
                        'env': env,
                        'data': df,
                        'type': 'training' if 'training' in csv_file.name else 'scan'
                    }
                    self.runs.append(run_info)
                    logger.info(f"å·²åŠ è½½: {csv_file.name} (ç®—æ³•: {algo}, ç±»å‹: {run_info['type']})")
                except Exception as e:
                    logger.error(f"åŠ è½½å¤±è´¥ {csv_file.name}: {e}")

    def plot_comparison(self, metric: str, data_type: str = 'training', x_axis: str = 'episode'):
        """
        å¯¹æ¯”ä¸åŒç®—æ³•åœ¨ç‰¹å®šæŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚
        """
        target_runs = [r for r in self.runs if r['type'] == data_type]
        if not target_runs:
            logger.warning(f"æ²¡æœ‰æ‰¾åˆ°ç±»å‹ä¸º {data_type} çš„æ•°æ®")
            return

        plt.figure(figsize=(14, 8))
        
        # è·å–ä¸­æ–‡å‹å¥½åç§°
        metric_zh = self.METRIC_NAME_MAP.get(metric, metric)
        x_axis_zh = self.METRIC_NAME_MAP.get(x_axis, x_axis)
        
        # æŒ‰ç®—æ³•åˆ†ç»„ç»˜å›¾
        unique_algos = sorted(set(r['algorithm'] for r in target_runs))
        
        for algo_id in unique_algos:
            algo_dfs = [r['data'] for r in target_runs if r['algorithm'] == algo_id]
            
            # åˆå¹¶è¯¥ç®—æ³•çš„æ‰€æœ‰è¿è¡Œæ•°æ®
            all_data = pd.concat(algo_dfs)
            
            # è·å–æ˜¾ç¤ºåç§°
            display_name = self.ALGO_NAME_MAP.get(algo_id, algo_id)
            
            if x_axis in all_data.columns and metric in all_data.columns:
                # ç»˜åˆ¶å‡å€¼çº¿å’Œæ ‡å‡†å·®å¡«å……åŒºåŸŸ
                sns.lineplot(
                    data=all_data, 
                    x=x_axis, 
                    y=metric, 
                    label=display_name, 
                    errorbar='sd',
                    linewidth=2.5
                )

        plt.title(f"å¤šç®—æ³•å¯¹æ¯”åˆ†æ: {metric_zh} éš {x_axis_zh} å˜åŒ–è¶‹åŠ¿", fontsize=16, pad=20)
        plt.xlabel(x_axis_zh, fontsize=12)
        plt.ylabel(metric_zh, fontsize=12)
        
        # ä¼˜åŒ–å›¾ä¾‹ï¼šæ”¾åœ¨å›¾å¤–å³ä¾§ï¼Œé¿å…é®æŒ¡æ›²çº¿
        plt.legend(title="ç®—æ³•ç±»å‹", title_fontsize='13', fontsize='11', bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
        
        plt.grid(True, which='both', linestyle='--', alpha=0.5)
        plt.tight_layout()
        
        filename = self.output_dir / f"comparison_{data_type}_{metric}.png"
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        logger.info(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {filename} (æ ‡ç­¾: {unique_algos})")
        plt.close()

    def generate_summary_report(self):
        """
        ç”Ÿæˆç»¼åˆå¯¹æ¯”æŠ¥å‘Š
        """
        summary_data = []
        for r in self.runs:
            df = r['data']
            algo_display = self.ALGO_NAME_MAP.get(r['algorithm'], r['algorithm'])
            
            if r['type'] == 'training':
                summary_data.append({
                    'ç®—æ³•åç§°': algo_display,
                    'è¿è¡Œè®°å½•': r['name'],
                    'å¹³å‡å¥–åŠ±': df['reward'].mean() if 'reward' in df.columns else 0,
                    'æœ€é«˜å¥–åŠ±': df['reward'].max() if 'reward' in df.columns else 0,
                    'è®­ç»ƒè½®æ¬¡': len(df),
                    'æœ€ç»ˆæ•ˆç‡': df['scan_efficiency'].iloc[-1] if 'scan_efficiency' in df.columns else 0
                })
            elif r['type'] == 'scan':
                summary_data.append({
                    'ç®—æ³•åç§°': algo_display,
                    'è¿è¡Œè®°å½•': r['name'],
                    'æœ€ç»ˆæ‰«æç‡(%)': df['scan_ratio'].iloc[-1] if 'scan_ratio' in df.columns else 0,
                    'æœ€ä½ç†µå€¼': df['global_avg_entropy'].min() if 'global_avg_entropy' in df.columns else 0,
                    'æ€»è€—æ—¶(s)': df['elapsed_time'].iloc[-1] if 'elapsed_time' in df.columns else 0
                })

        summary_df = pd.DataFrame(summary_data)
        if not summary_df.empty:
            # æŒ‰ç®—æ³•èšåˆçœ‹å‡å€¼
            algo_comparison = summary_df.groupby('ç®—æ³•åç§°').mean(numeric_only=True)
            print("\n" + "="*70)
            print("ğŸš€ å¤šç®—æ³•å¹³å‡æ€§èƒ½é‡åŒ–å¯¹æ¯”æŠ¥å‘Š (Averaged Performance Report)")
            print("="*70)
            print(algo_comparison)
            print("="*70)
            
            report_file = self.output_dir / "algorithm_comparison_report.csv"
            # å¯¼å‡ºå¸¦ä¸­æ–‡è¡¨å¤´çš„ CSVï¼Œå¹¶ä½¿ç”¨ UTF-8 SIG ç¡®ä¿ Excel æ‰“å¼€ä¸ä¹±ç 
            algo_comparison.to_csv(report_file, encoding='utf-8-sig')
            logger.info(f"å¯¹æ¯”æŠ¥å‘Šå·²å¯¼å‡º: {report_file}")
        else:
            logger.warning("æ²¡æœ‰å¯ç”¨äºç”ŸæˆæŠ¥å‘Šçš„æ•°æ®")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="å¤šç®—æ³•å¯¹æ¯”åˆ†æå·¥å…·")
    parser.add_argument("--dirs", nargs='+', help="æ•°æ®ç›®å½•åˆ—è¡¨", default=[
        "multirotor/DDPG_Weight/airsim_training_logs",
        "multirotor/DQN_Movement/logs/dqn_scan_data"
    ])
    parser.add_argument("--out", default="multirotor/Algorithm/analysis_results", help="ç»“æœä¿å­˜ç›®å½•")
    args = parser.parse_args()

    analyzer = UnifiedTrainingAnalyzer(output_dir=args.out)
    analyzer.load_data(args.dirs)
    
    # ç»˜åˆ¶å…³é”®æŒ‡æ ‡å¯¹æ¯”
    # 1. è®­ç»ƒå¥–åŠ±å¯¹æ¯” (DQN vs DDPG vs HRL)
    analyzer.plot_comparison(metric='reward', data_type='training', x_axis='episode')
    
    # 2. æ‰«ææ•ˆç‡å¯¹æ¯”
    analyzer.plot_comparison(metric='scan_efficiency', data_type='training', x_axis='episode')
    
    # 3. å®æ—¶æ‰«ææ¯”ä¾‹å¯¹æ¯” (éšæ—¶é—´å˜åŒ–)
    analyzer.plot_comparison(metric='scan_ratio', data_type='scan', x_axis='elapsed_time')
    
    # 4. ç†µä¸‹é™é€Ÿåº¦å¯¹æ¯”
    analyzer.plot_comparison(metric='global_avg_entropy', data_type='scan', x_axis='elapsed_time')
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_summary_report()
