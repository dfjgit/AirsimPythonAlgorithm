# DQNä½¿ç”¨æŒ‡å—

AirSimæ— äººæœºä»¿çœŸç³»ç»Ÿçš„DQNå¼ºåŒ–å­¦ä¹ æ¨¡å—å®Œæ•´ä½¿ç”¨æŒ‡å—

---

## ğŸ“‹ æ¦‚è¿°

æœ¬é¡¹ç›®æä¾›äº†ä¸¤ç§DQNå¼ºåŒ–å­¦ä¹ å®ç°æ–¹æ¡ˆï¼Œåˆ†åˆ«é’ˆå¯¹ä¸åŒçš„æ§åˆ¶ç­–ç•¥ï¼š

### ğŸš DQNç§»åŠ¨æ§åˆ¶ (DQN_Movement)
- **ç®—æ³•**: DQN (Deep Q-Network)
- **åŠ¨ä½œç©ºé—´**: 6ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆä¸Š/ä¸‹/å·¦/å³/å‰/åï¼‰
- **è§‚å¯Ÿç©ºé—´**: 21ç»´çŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ã€ç†µå€¼ã€Leaderä¿¡æ¯ç­‰ï¼‰
- **æ§åˆ¶æ–¹å¼**: AIç›´æ¥å†³ç­–ä¸‹ä¸€æ­¥å¾€å“ªä¸ªæ–¹å‘ç§»åŠ¨

### âš–ï¸ DQNæƒé‡å­¦ä¹  (DQN_Weight)
- **ç®—æ³•**: DDPG (Deep Deterministic Policy Gradient)
- **åŠ¨ä½œç©ºé—´**: 5ä¸ªè¿ç»­æƒé‡ç³»æ•°ï¼ˆÎ±1-Î±5: æ’æ–¥åŠ›ã€ç†µå€¼ã€è·ç¦»ã€LeaderèŒƒå›´ã€æ–¹å‘ä¿æŒï¼‰
- **è§‚å¯Ÿç©ºé—´**: 18ç»´çŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ã€ç†µå€¼ã€Leaderä¿¡æ¯ç­‰ï¼‰
- **æ§åˆ¶æ–¹å¼**: AIåŠ¨æ€è°ƒæ•´APFç®—æ³•çš„5ä¸ªæƒé‡å‚æ•°ï¼Œé—´æ¥å½±å“ç§»åŠ¨è¡Œä¸º

---

## ğŸ†š ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | DQN_Movement | DQN_Weight |
|------|-------------|-----------|
| **ç®—æ³•** | DQN | DDPG |
| **åŠ¨ä½œç±»å‹** | ç¦»æ•£åŠ¨ä½œï¼ˆ6ä¸ªæ–¹å‘ï¼‰ | è¿ç»­åŠ¨ä½œï¼ˆ5ä¸ªæƒé‡ï¼‰ |
| **åŠ¨ä½œç©ºé—´** | `Discrete(6)` | `Box(5)` |
| **æ§åˆ¶å±‚çº§** | åº•å±‚ï¼ˆç›´æ¥æ§åˆ¶ç§»åŠ¨ï¼‰ | é«˜å±‚ï¼ˆè°ƒæ•´è¡Œä¸ºç­–ç•¥ï¼‰ |
| **å“åº”é€Ÿåº¦** | å¿«é€Ÿç›´æ¥ | é€šè¿‡APFé—´æ¥å“åº” |
| **çµæ´»æ€§** | å›ºå®š6ä¸ªæ–¹å‘ | æ— é™è¿ç»­æƒé‡ç»„åˆ |
| **å¤æ‚åº¦** | ç›¸å¯¹ç®€å• | ç›¸å¯¹å¤æ‚ |
| **è®­ç»ƒæ—¶é—´** | 10-30åˆ†é’Ÿ | 15-40åˆ†é’Ÿ |
| **å¯è§£é‡Šæ€§** | è¾ƒä½ï¼ˆé»‘ç›’å†³ç­–ï¼‰ | è¾ƒé«˜ï¼ˆå¯è§‚å¯Ÿæƒé‡å˜åŒ–ï¼‰ |
| **ä¸APFç»“åˆ** | ç‹¬ç«‹äºAPF | å¢å¼ºAPFç®—æ³• |

---

## ğŸ¯ é€‰æ‹©æŒ‡å—

### é€‰æ‹© DQN_Movementï¼ˆç§»åŠ¨æ§åˆ¶ï¼‰å¦‚æœï¼š
- âœ… ä½ æƒ³ç›´æ¥å­¦ä¹ ç§»åŠ¨ç­–ç•¥
- âœ… åœºæ™¯ç›¸å¯¹ç®€å•
- âœ… éœ€è¦å¿«é€ŸåŸå‹éªŒè¯
- âœ… ç¬¬ä¸€æ¬¡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ 

### é€‰æ‹© DQN_Weightï¼ˆæƒé‡APFï¼‰å¦‚æœï¼š
- âœ… å·²æœ‰APFç®—æ³•æ¡†æ¶
- âœ… éœ€è¦è°ƒæ•´å¤æ‚è¡Œä¸º
- âœ… åœºæ™¯éœ€è¦ç²¾ç»†æ§åˆ¶
- âœ… è¿½æ±‚æ›´ä¼˜çš„æ€§èƒ½

---

## ğŸš DQNç§»åŠ¨æ§åˆ¶è¯¦ç»†æŒ‡å—

### ğŸ“ æ–‡ä»¶ç»“æ„
```
DQN_Movement/
â”œâ”€â”€ movement_env.py                    # æ— äººæœºç§»åŠ¨ç¯å¢ƒç±»ï¼ˆæ ¸å¿ƒï¼‰â­
â”œâ”€â”€ movement_dqn_config.json          # é…ç½®æ–‡ä»¶ï¼ˆå¥–åŠ±ã€é˜ˆå€¼ã€è®­ç»ƒå‚æ•°ï¼‰
â”œâ”€â”€ train_movement_dqn.py             # çº¯æ¨¡æ‹Ÿè®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_movement_with_airsim.py    # AirSimé›†æˆè®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_movement_dqn.py              # æ¨¡å‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements_movement.txt         # Pythonä¾èµ–
â”œâ”€â”€ logs/                             # è®­ç»ƒæ—¥å¿—ç›®å½•
â””â”€â”€ models/                           # è®­ç»ƒæ¨¡å‹ä¿å­˜ç›®å½•
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒå‡†å¤‡
```bash
pip install torch stable-baselines3 numpy gymnasium
```

#### 2. çº¯æ¨¡æ‹Ÿè®­ç»ƒï¼ˆæ¨èå…¥é—¨ï¼‰
```bash
# Windows
train_movement_dqn.bat

# æˆ–è€…ç›´æ¥è¿è¡ŒPython
python multirotor/DQN_Movement/train_movement_dqn.py
```

#### 3. AirSimé›†æˆè®­ç»ƒï¼ˆçœŸå®ç¯å¢ƒï¼‰
```bash
# 1. å…ˆå¯åŠ¨Unityå®¢æˆ·ç«¯
# 2. è¿è¡Œè®­ç»ƒè„šæœ¬
train_movement_with_airsim.bat
```

#### 4. æµ‹è¯•æ¨¡å‹
```bash
test_movement_dqn.bat
```

### âš™ï¸ é…ç½®è¯´æ˜

#### ç§»åŠ¨å‚æ•°
```json
{
  "step_size": 1.0,        // æ¯æ­¥ç§»åŠ¨è·ç¦»ï¼ˆç±³ï¼‰
  "max_steps": 500         // æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
}
```

#### å¥–åŠ±è®¾è®¡
```json
{
  "exploration": 10.0,          // å‘ç°æ–°åŒºåŸŸå¥–åŠ±
  "entropy_reduction": 5.0,     // é™ä½ç†µå€¼å¥–åŠ±
  "collision": -50.0,           // ç¢°æ’æƒ©ç½š
  "out_of_range": -30.0,        // è¶Šç•Œæƒ©ç½š
  "smooth_movement": 1.0,       // å¹³æ»‘ç§»åŠ¨å¥–åŠ±
  "step_penalty": -0.1,         // æ¯æ­¥å°æƒ©ç½š
  "success": 100.0              // å®Œæˆä»»åŠ¡å¤§å¥–åŠ±
}
```

#### è®­ç»ƒå‚æ•°
```json
{
  "total_timesteps": 100000,           // è®­ç»ƒæ€»æ­¥æ•°
  "learning_rate": 0.0001,            // å­¦ä¹ ç‡
  "buffer_size": 50000,               // ç»éªŒå›æ”¾ç¼“å†²åŒº
  "batch_size": 32,                   // æ‰¹æ¬¡å¤§å°
  "gamma": 0.99,                      // æŠ˜æ‰£å› å­
  "exploration_fraction": 0.3,        // æ¢ç´¢è¡°å‡æ¯”ä¾‹
  "exploration_initial_eps": 1.0,     // åˆå§‹æ¢ç´¢ç‡
  "exploration_final_eps": 0.05       // æœ€ç»ˆæ¢ç´¢ç‡
}
```

### ğŸ“Š çŠ¶æ€ç©ºé—´è¯¦è§£

ç¯å¢ƒè§‚å¯Ÿç©ºé—´ä¸º21ç»´å‘é‡ï¼š

| ç»´åº¦èŒƒå›´ | åç§° | è¯´æ˜ |
|---------|------|------|
| 0-2 | ä½ç½® | x, y, zåæ ‡ |
| 3-5 | é€Ÿåº¦ | vx, vy, vzé€Ÿåº¦åˆ†é‡ |
| 6-8 | æœå‘ | forwardå‘é‡ |
| 9-11 | å±€éƒ¨ç†µå€¼ | å¹³å‡ç†µã€æœ€å¤§ç†µã€ç†µæ ‡å‡†å·® |
| 12-14 | Leaderç›¸å¯¹ä½ç½® | dx, dy, dz |
| 15-16 | LeaderèŒƒå›´ä¿¡æ¯ | è·ç¦»ã€æ˜¯å¦è¶Šç•Œ |
| 17-19 | æ‰«æè¿›åº¦ | å·²æ‰«ææ¯”ä¾‹ã€æ•°é‡ã€å‰©ä½™ |
| 20 | æœ€è¿‘æ— äººæœºè·ç¦» | é¿éšœç”¨ |

### ğŸ® åŠ¨ä½œç©ºé—´è¯¦è§£

6ä¸ªç¦»æ•£åŠ¨ä½œï¼š

| åŠ¨ä½œID | æ–¹å‘ | ä½ç§»å‘é‡ |
|--------|------|---------|
| 0 | ä¸Š | (0, 0, +step_size) |
| 1 | ä¸‹ | (0, 0, -step_size) |
| 2 | å·¦ | (-step_size, 0, 0) |
| 3 | å³ | (+step_size, 0, 0) |
| 4 | å‰ | (0, +step_size, 0) |
| 5 | å | (0, -step_size, 0) |

---

## âš–ï¸ DQNæƒé‡å­¦ä¹ è¯¦ç»†æŒ‡å—

### ğŸ“ æ–‡ä»¶ç»“æ„
```
DQN_Weight/
â”œâ”€â”€ simple_weight_env.py    # æƒé‡å­¦ä¹ ç¯å¢ƒ
â”œâ”€â”€ train_simple.py         # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_trained_model.py   # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ dqn_reward_config.json  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ models/                 # è®­ç»ƒæ¨¡å‹ä¿å­˜ç›®å½•
â””â”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—ç›®å½•
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. çº¯æ¨¡æ‹Ÿè®­ç»ƒ
```bash
cd multirotor/DQN_Weight
python train_simple.py
```

#### 2. AirSimé›†æˆè®­ç»ƒ
```bash
python train_with_airsim.py
```

#### 3. æµ‹è¯•æ¨¡å‹
```bash
python test_trained_model.py
```

### âš™ï¸ æƒé‡å¹³è¡¡æœºåˆ¶

#### é—®é¢˜æè¿°
DQNæ¨¡å‹å¯èƒ½å­¦ä¹ åˆ°æç«¯çš„æƒé‡ç»„åˆï¼Œå¯¼è‡´ï¼š
- æŸä¸ªæƒé‡å ä¸»å¯¼åœ°ä½
- APFç®—æ³•å¤±è¡¡
- æ— äººæœºè¡Œä¸ºå¼‚å¸¸

#### è§£å†³æ–¹æ¡ˆ

**1. ç¼©å°æƒé‡èŒƒå›´**
```python
# åŠ¨ä½œç©ºé—´
self.action_space = spaces.Box(
    low=0.5,   # æœ€å°æƒé‡
    high=5.0,  # æœ€å¤§æƒé‡
    shape=(5,),
    dtype=np.float32
)
```

**2. è½¯å½’ä¸€åŒ–**
```python
action_mean = np.mean(action)
action_std = np.std(action)

# å¦‚æœæ ‡å‡†å·®è¿‡å¤§ï¼ˆ>1.5ï¼‰ï¼Œè¿›è¡Œå¹³æ»‘
if action_std > 1.5:
    # å°†æç«¯å€¼æ‹‰å›åˆ°å‡å€¼é™„è¿‘ï¼ˆä¿ç•™70%çš„å·®å¼‚ï¼‰
    action = action_mean + (action - action_mean) * 0.7
    action = np.clip(action, 0.5, 5.0)
```

**3. æ¯”ä¾‹é™åˆ¶**
```python
min_weight = np.min(action)
max_weight = np.max(action)

# æœ€å¤§å€¼ä¸è¶…è¿‡æœ€å°å€¼çš„5å€
if max_weight > min_weight * 5:
    scale = (min_weight * 5) / max_weight
    action = action * scale
    action = np.clip(action, 0.5, 5.0)
```

### ğŸ“Š æƒé‡å¹³è¡¡æµç¨‹

```
æ¨¡å‹è¾“å‡ºåŸå§‹æƒé‡
    â†“
èŒƒå›´è£å‰ª [0.5, 5.0]
    â†“
è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    â†“
æ ‡å‡†å·® > 1.5?
    â”œâ”€ æ˜¯ â†’ è½¯å½’ä¸€åŒ–ï¼ˆæ‹‰å›å‡å€¼ï¼‰
    â””â”€ å¦ â†’ ç»§ç»­
    â†“
è®¡ç®—æœ€å¤§/æœ€å°æ¯”ä¾‹
    â†“
æ¯”ä¾‹ > 5?
    â”œâ”€ æ˜¯ â†’ ç¼©æ”¾æƒé‡
    â””â”€ å¦ â†’ ç»§ç»­
    â†“
æœ€ç»ˆå¹³è¡¡æƒé‡
    â†“
åº”ç”¨åˆ°APFç®—æ³•
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### ä½¿ç”¨TensorboardæŸ¥çœ‹è®­ç»ƒæ›²çº¿

```bash
# çº¯æ¨¡æ‹Ÿè®­ç»ƒæ—¥å¿—
tensorboard --logdir=multirotor/DQN_Movement/logs/movement_dqn/

# AirSimé›†æˆè®­ç»ƒæ—¥å¿—
tensorboard --logdir=multirotor/DQN_Movement/logs/movement_dqn_airsim/

# æƒé‡DQNè®­ç»ƒæ—¥å¿—
tensorboard --logdir=multirotor/DQN_Weight/logs/
```

### å…³é”®æŒ‡æ ‡

- **ep_rew_mean**: å¹³å‡episodeå¥–åŠ±ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **ep_len_mean**: å¹³å‡episodeé•¿åº¦ï¼ˆå¤ªé•¿è¯´æ˜æ•ˆç‡ä½ï¼‰
- **exploration_rate**: æ¢ç´¢ç‡ï¼ˆé€æ¸è¡°å‡åˆ°0.05ï¼‰
- **loss**: è®­ç»ƒæŸå¤±ï¼ˆåº”è¯¥é€æ¸ä¸‹é™ï¼‰

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: 
- ä½¿ç”¨GPU: ç¡®ä¿å®‰è£…äº†PyTorch GPUç‰ˆæœ¬
- å‡å°‘è®­ç»ƒæ­¥æ•°: ä¿®æ”¹ `total_timesteps`
- å¢å¤§æ‰¹æ¬¡: ä¿®æ”¹ `batch_size` (éœ€è¦æ›´å¤šå†…å­˜)
- ä½¿ç”¨çº¯æ¨¡æ‹Ÿæ¨¡å¼: ä¸è¿æ¥Unity

### Q2: æ¨¡å‹æ€§èƒ½ä¸å¥½ï¼Ÿ

**A**:
- è°ƒæ•´å¥–åŠ±æƒé‡: å¢åŠ æ¢ç´¢å¥–åŠ±ï¼Œå‡å°‘æƒ©ç½š
- å¢åŠ è®­ç»ƒæ—¶é—´: æé«˜ `total_timesteps`
- è°ƒæ•´æ¢ç´¢ç‡: å»¶é•¿æ¢ç´¢æ—¶é—´ `exploration_fraction`
- æ£€æŸ¥æ•°æ®: ç¡®ä¿ç¯å¢ƒçŠ¶æ€æ­£ç¡®

### Q3: å¦‚ä½•ç»§ç»­è®­ç»ƒå·²æœ‰æ¨¡å‹ï¼Ÿ

**A**:
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä¼šè‡ªåŠ¨æ£€æµ‹
# ä¹Ÿå¯ä»¥æ‰‹åŠ¨åŠ è½½
model = DQN.load("models/movement_dqn_final.zip", env=env)
model.learn(total_timesteps=50000)  # ç»§ç»­è®­ç»ƒ
```

### Q4: æƒé‡é¢„æµ‹å¤±è¡¡æ€ä¹ˆåŠï¼Ÿ

**A**:
- æ£€æŸ¥æƒé‡å¹³è¡¡æœºåˆ¶æ˜¯å¦å¯ç”¨
- è°ƒæ•´å¹³è¡¡å‚æ•°ï¼ˆæ ‡å‡†å·®é˜ˆå€¼ã€æ¯”ä¾‹é™åˆ¶ï¼‰
- æŸ¥çœ‹å¯è§†åŒ–é¢æ¿ä¸­çš„æƒé‡æ¡
- é‡æ–°è®­ç»ƒæ¨¡å‹

---

## ğŸ“ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

ç¼–è¾‘ç¯å¢ƒæ–‡ä»¶ä¸­çš„ `_calculate_reward` æ–¹æ³•ï¼š

```python
def _calculate_reward(self, action, prev_state, next_state):
    reward = 0.0
    
    # æ·»åŠ è‡ªå®šä¹‰å¥–åŠ±
    # ä¾‹å¦‚ï¼šé¼“åŠ±å‘é«˜ç†µåŒºåŸŸç§»åŠ¨
    entropy_diff = next_state[9] - prev_state[9]  # ç¬¬9ç»´æ˜¯å¹³å‡ç†µ
    if entropy_diff > 0:
        reward += 2.0  # ç§»å‘é«˜ç†µåŒºåŸŸ
    
    # ... å…¶ä»–å¥–åŠ±è®¡ç®—
    return reward
```

### ä½¿ç”¨ä¸åŒçš„ç®—æ³•

Stable-Baselines3æ”¯æŒå¤šç§ç®—æ³•ï¼š

```python
from stable_baselines3 import DQN, A2C, PPO

# ä½¿ç”¨PPOä»£æ›¿DQN
model = PPO("MlpPolicy", env, ...)
```

### å¹¶è¡Œè®­ç»ƒ

```python
from stable_baselines3.common.vec_env import SubprocVecEnv

# åˆ›å»ºå¤šä¸ªå¹¶è¡Œç¯å¢ƒ
def make_env():
    return MovementEnv(server=None, drone_name="UAV1")

env = SubprocVecEnv([make_env for _ in range(4)])  # 4ä¸ªå¹¶è¡Œç¯å¢ƒ
model = DQN("MlpPolicy", env, ...)
```

---

## ğŸ“ ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ

### åœ¨AlgorithmServerä¸­ä½¿ç”¨

åœ¨ `AlgorithmServer.py` ä¸­é›†æˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼š

```python
from stable_baselines3 import DQN

class MultiDroneAlgorithmServer:
    def __init__(self, use_dqn_movement=False):
        # ...
        if use_dqn_movement:
            self.dqn_model = DQN.load("path/to/movement_dqn_final.zip")
    
    def _control_loop(self, drone_name):
        # ä½¿ç”¨DQNé¢„æµ‹åŠ¨ä½œ
        obs = self._get_observation(drone_name)
        action = self.dqn_model.predict(obs, deterministic=True)[0]
        self._apply_action(drone_name, action)
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [Stable-Baselines3 æ–‡æ¡£](https://stable-baselines3.readthedocs.io/)
- [DQNåŸè®ºæ–‡](https://arxiv.org/abs/1312.5602)
- [DDPGåŸè®ºæ–‡](https://arxiv.org/abs/1509.02971)
- [OpenAI Gym æ–‡æ¡£](https://www.gymlibrary.dev/)

---

## ğŸ¤ è´¡çŒ®

å¦‚æœæ‚¨æœ‰æ”¹è¿›å»ºè®®æˆ–å‘ç°é—®é¢˜ï¼Œæ¬¢è¿æå‡ºIssueæˆ–Pull Requestã€‚

---

**æœ€åæ›´æ–°**: 2025-01-16
